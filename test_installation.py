
#!/usr/bin/env python3
"""
å®‰è£…æµ‹è¯•è„šæœ¬
éªŒè¯RTX 3060 Tiç¯å¢ƒé…ç½®æ˜¯å¦æ­£ç¡®
"""

import sys
import traceback
import time

def print_header(title):
    """æ‰“å°æ ‡é¢˜"""
    print("\n" + "=" * 60)
    print(f" {title}")
    print("=" * 60)

def print_result(test_name, success, details=""):
    """æ‰“å°æµ‹è¯•ç»“æœ"""
    status = "âœ… é€šè¿‡" if success else "âŒ å¤±è´¥"
    print(f"{test_name:<30} {status}")
    if details:
        print(f"   è¯¦æƒ…: {details}")

def test_python_version():
    """æµ‹è¯•Pythonç‰ˆæœ¬"""
    print_header("ğŸ Pythonç¯å¢ƒæµ‹è¯•")
    
    version_info = sys.version_info
    python_version = f"{version_info.major}.{version_info.minor}.{version_info.micro}"
    print(f"Pythonç‰ˆæœ¬: {python_version}")
    
    success = version_info.major == 3 and 8 <= version_info.minor <= 11
    print_result("Pythonç‰ˆæœ¬æ£€æŸ¥", success, 
                "éœ€è¦Python 3.8-3.11" if not success else f"ç‰ˆæœ¬ {python_version}")
    return success

def test_torch_cuda():
    """æµ‹è¯•PyTorchå’ŒCUDA"""
    print_header("ğŸ”¥ PyTorchå’ŒCUDAæµ‹è¯•")
    
    try:
        import torch
        pytorch_version = torch.__version__
        print(f"PyTorchç‰ˆæœ¬: {pytorch_version}")
        
        cuda_available = torch.cuda.is_available()
        print_result("PyTorchå®‰è£…", True, f"ç‰ˆæœ¬ {pytorch_version}")
        print_result("CUDAå¯ç”¨æ€§", cuda_available)
        
        if cuda_available:
            cuda_version = torch.version.cuda
            gpu_count = torch.cuda.device_count()
            print(f"CUDAç‰ˆæœ¬: {cuda_version}")
            print(f"GPUæ•°é‡: {gpu_count}")
            
            for i in range(gpu_count):
                props = torch.cuda.get_device_properties(i)
                gpu_name = props.name
                gpu_memory = props.total_memory / 1024**3
                print(f"GPU {i}: {gpu_name} ({gpu_memory:.1f} GB)")
                
                if "3060 Ti" in gpu_name:
                    print_result("RTX 3060 Tiæ£€æµ‹", True, gpu_name)
                    
            # æµ‹è¯•GPUè®¡ç®—
            try:
                x = torch.randn(1000, 1000).cuda()
                y = torch.matmul(x, x)
                torch.cuda.synchronize()
                print_result("GPUè®¡ç®—æµ‹è¯•", True)
                
                # æ¸…ç†GPUå†…å­˜
                del x, y
                torch.cuda.empty_cache()
                
            except Exception as e:
                print_result("GPUè®¡ç®—æµ‹è¯•", False, str(e))
                return False
                
        return cuda_available
        
    except ImportError:
        print_result("PyTorchå®‰è£…", False, "PyTorchæœªå®‰è£…")
        return False
    except Exception as e:
        print_result("PyTorchæµ‹è¯•", False, str(e))
        return False

def test_whisper_models():
    """æµ‹è¯•Whisperæ¨¡å‹"""
    print_header("ğŸ¤ Whisperæ¨¡å‹æµ‹è¯•")
    
    results = []
    
    # æµ‹è¯•OpenAI Whisper
    try:
        import whisper
        print_result("OpenAI Whisper", True)
        results.append(True)
    except ImportError:
        print_result("OpenAI Whisper", False, "æœªå®‰è£…")
        results.append(False)
    
    # æµ‹è¯•Faster-Whisper
    try:
        from faster_whisper import WhisperModel
        print_result("Faster-Whisper", True)
        results.append(True)
    except ImportError:
        print_result("Faster-Whisper", False, "æœªå®‰è£…")
        results.append(False)
    
    # æµ‹è¯•Transformers
    try:
        from transformers import pipeline
        print_result("Transformers", True)
        results.append(True)
    except ImportError:
        print_result("Transformers", False, "æœªå®‰è£…")
        results.append(False)
    
    return any(results)

def test_audio_video_libs():
    """æµ‹è¯•éŸ³è§†é¢‘å¤„ç†åº“"""
    print_header("ğŸ”Š éŸ³è§†é¢‘å¤„ç†åº“æµ‹è¯•")
    
    results = []
    
    # æµ‹è¯•éŸ³é¢‘åº“
    try:
        import librosa
        print_result("Librosa", True)
        results.append(True)
    except ImportError:
        print_result("Librosa", False, "æœªå®‰è£…")
        results.append(False)
    
    try:
        import soundfile
        print_result("SoundFile", True)
        results.append(True)
    except ImportError:
        print_result("SoundFile", False, "æœªå®‰è£…")
        results.append(False)
    
    try:
        import numpy
        numpy_version = numpy.__version__
        print_result("NumPy", True, f"ç‰ˆæœ¬ {numpy_version}")
        results.append(True)
    except ImportError:
        print_result("NumPy", False, "æœªå®‰è£…")
        results.append(False)
    
    # æµ‹è¯•è§†é¢‘åº“
    try:
        import moviepy
        print_result("MoviePy", True)
        results.append(True)
    except ImportError:
        print_result("MoviePy", False, "æœªå®‰è£…ï¼Œå°†ä½¿ç”¨FFmpeg")
        # æµ‹è¯•FFmpeg
        import subprocess
        try:
            result = subprocess.run(["ffmpeg", "-version"], 
                                  capture_output=True, text=True, timeout=5)
            if result.returncode == 0:
                print_result("FFmpeg", True)
                results.append(True)
            else:
                print_result("FFmpeg", False, "FFmpegä¸å¯ç”¨")
                results.append(False)
        except (FileNotFoundError, subprocess.TimeoutExpired):
            print_result("FFmpeg", False, "FFmpegæœªå®‰è£…")
            results.append(False)
    
    return any(results)

def test_optional_acceleration():
    """æµ‹è¯•å¯é€‰åŠ é€Ÿåº“"""
    print_header("âš¡ å¯é€‰åŠ é€Ÿåº“æµ‹è¯•")
    
    # æµ‹è¯•TensorRT
    try:
        import tensorrt
        trt_version = tensorrt.__version__
        print_result("TensorRT", True, f"ç‰ˆæœ¬ {trt_version}")
    except ImportError:
        print_result("TensorRT", False, "æœªå®‰è£…ï¼ˆå¯é€‰ï¼‰")
    
    # æµ‹è¯•PyCuda
    try:
        import pycuda
        print_result("PyCuda", True)
    except ImportError:
        print_result("PyCuda", False, "æœªå®‰è£…ï¼ˆTensorRTéœ€è¦ï¼‰")

def test_model_download():
    """æµ‹è¯•æ¨¡å‹ä¸‹è½½"""
    print_header("ğŸ“¥ æ¨¡å‹ä¸‹è½½æµ‹è¯•")
    
    try:
        print("æµ‹è¯•ä¸‹è½½tinyæ¨¡å‹ï¼ˆæœ€å°æ¨¡å‹ï¼Œç”¨äºéªŒè¯ï¼‰...")
        import whisper
        
        # ä¸‹è½½æœ€å°çš„æ¨¡å‹è¿›è¡Œæµ‹è¯•
        model = whisper.load_model("tiny", download_root="./models")
        print_result("æ¨¡å‹ä¸‹è½½", True, "tinyæ¨¡å‹ä¸‹è½½æˆåŠŸ")
        
        # ç®€å•æµ‹è¯•æ¨¡å‹åŠ è½½
        if hasattr(model, 'dims'):
            print_result("æ¨¡å‹åŠ è½½", True, f"æ¨¡å‹ç»´åº¦: {model.dims}")
        else:
            print_result("æ¨¡å‹åŠ è½½", True)
            
        del model  # é‡Šæ”¾å†…å­˜
        return True
        
    except Exception as e:
        print_result("æ¨¡å‹ä¸‹è½½", False, str(e))
        return False

def test_file_system():
    """æµ‹è¯•æ–‡ä»¶ç³»ç»Ÿ"""
    print_header("ğŸ“ æ–‡ä»¶ç³»ç»Ÿæµ‹è¯•")
    
    import os
    
    # æµ‹è¯•å·¥ä½œç›®å½•åˆ›å»º
    test_dirs = ["models", "temp", "output"]
    results = []
    
    for dir_name in test_dirs:
        try:
            os.makedirs(dir_name, exist_ok=True)
            if os.path.exists(dir_name) and os.access(dir_name, os.W_OK):
                print_result(f"{dir_name}ç›®å½•", True)
                results.append(True)
            else:
                print_result(f"{dir_name}ç›®å½•", False, "æ— å†™å…¥æƒé™")
                results.append(False)
        except Exception as e:
            print_result(f"{dir_name}ç›®å½•", False, str(e))
            results.append(False)
    
    return all(results)

def performance_benchmark():
    """æ€§èƒ½åŸºå‡†æµ‹è¯•"""
    print_header("ğŸš€ æ€§èƒ½åŸºå‡†æµ‹è¯•")
    
    try:
        import torch
        import time
        
        if not torch.cuda.is_available():
            print("è·³è¿‡GPUæ€§èƒ½æµ‹è¯•ï¼ˆCUDAä¸å¯ç”¨ï¼‰")
            return
            
        # GPUå†…å­˜æµ‹è¯•
        device = torch.device('cuda')
        
        print("æµ‹è¯•GPUå†…å­˜åˆ†é…...")
        start_time = time.time()
        
        # åˆ†é…1GBå†…å­˜æµ‹è¯•
        tensor_size = (1024, 1024, 256)  # çº¦1GB float32
        try:
            test_tensor = torch.randn(tensor_size, device=device)
            allocation_time = time.time() - start_time
            print_result("1GBå†…å­˜åˆ†é…", True, f"{allocation_time:.2f}ç§’")
            
            # é‡Šæ”¾å†…å­˜
            del test_tensor
            torch.cuda.empty_cache()
            
        except torch.cuda.OutOfMemoryError:
            print_result("1GBå†…å­˜åˆ†é…", False, "æ˜¾å­˜ä¸è¶³")
            
        # ç®€å•è®¡ç®—æ€§èƒ½æµ‹è¯•
        print("æµ‹è¯•GPUè®¡ç®—æ€§èƒ½...")
        start_time = time.time()
        
        a = torch.randn(2048, 2048, device=device)
        b = torch.randn(2048, 2048, device=device)
        c = torch.matmul(a, b)
        torch.cuda.synchronize()
        
        compute_time = time.time() - start_time
        print_result("çŸ©é˜µä¹˜æ³•æµ‹è¯•", True, f"{compute_time:.2f}ç§’")
        
        # æ¸…ç†
        del a, b, c
        torch.cuda.empty_cache()
        
    except Exception as e:
        print_result("æ€§èƒ½æµ‹è¯•", False, str(e))

def main():
    """ä¸»æµ‹è¯•å‡½æ•°"""
    print("ğŸ¯ ä¸­æ–‡ç”µè§†å‰§éŸ³é¢‘è½¬æ–‡å­—å·¥å…· - å®‰è£…éªŒè¯")
    print("æ­¤æµ‹è¯•å°†éªŒè¯RTX 3060 Tiç¯å¢ƒé…ç½®æ˜¯å¦æ­£ç¡®")
    
    test_results = []
    
    # æ ¸å¿ƒæµ‹è¯•
    test_results.append(test_python_version())
    test_results.append(test_torch_cuda())
    test_results.append(test_whisper_models())
    test_results.append(test_audio_video_libs())
    test_results.append(test_file_system())
    
    # å¯é€‰æµ‹è¯•
    test_optional_acceleration()
    test_results.append(test_model_download())
    
    # æ€§èƒ½æµ‹è¯•
    performance_benchmark()
    
    # æ€»ç»“
    print_header("ğŸ“Š æµ‹è¯•ç»“æœæ€»ç»“")
    
    passed_tests = sum(test_results)
    total_tests = len(test_results)
    
    print(f"é€šè¿‡æµ‹è¯•: {passed_tests}/{total_tests}")
    
    if passed_tests == total_tests:
        print("\nğŸ‰ æ­å–œï¼æ‰€æœ‰æ ¸å¿ƒæµ‹è¯•é€šè¿‡ï¼")
        print("âœ… ç³»ç»Ÿå·²å‡†å¤‡å°±ç»ªï¼Œå¯ä»¥å¼€å§‹ä½¿ç”¨è§†é¢‘è½¬å­—å¹•å·¥å…·")
        print("\nğŸ“– ä½¿ç”¨æ–¹æ³•ï¼š")
        print("1. å›¾å½¢ç•Œé¢ï¼šåŒå‡»'å¯åŠ¨è½¬æ¢å·¥å…·.bat'")
        print("2. å‘½ä»¤è¡Œï¼špython main.py ä½ çš„è§†é¢‘.mp4 --model faster-base")
        print("3. å¿«é€Ÿè½¬æ¢ï¼šåŒå‡»'å¿«é€Ÿè½¬æ¢.bat'")
    else:
        print("\nâš ï¸ éƒ¨åˆ†æµ‹è¯•å¤±è´¥ï¼Œç³»ç»Ÿå¯èƒ½æ— æ³•æ­£å¸¸å·¥ä½œ")
        print("â— è¯·è¿è¡Œ'ä¸€é”®å®‰è£…è„šæœ¬.bat'é‡æ–°å®‰è£…ä¾èµ–")
        print("ğŸ’¡ æˆ–è€…æ£€æŸ¥NVIDIAé©±åŠ¨å’ŒCUDAç¯å¢ƒ")
    
    print(f"\nğŸ“ è¯¦ç»†æ—¥å¿—å·²ä¿å­˜åˆ°å½“å‰ç›®å½•")
    print("ğŸ†˜ å¦‚æœ‰é—®é¢˜è¯·æŸ¥çœ‹ä¸Šæ–¹çš„æµ‹è¯•ç»“æœ")

if __name__ == "__main__":
    try:
        main()
    except KeyboardInterrupt:
        print("\n\nâ¹ï¸ ç”¨æˆ·ä¸­æ–­æµ‹è¯•")
    except Exception as e:
        print(f"\n\nâŒ æµ‹è¯•è¿‡ç¨‹ä¸­å‘ç”Ÿæ„å¤–é”™è¯¯:")
        print(f"é”™è¯¯: {e}")
        traceback.print_exc()
    finally:
        print("\næŒ‰å›è½¦é”®é€€å‡º...")
        input()
