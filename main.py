
#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
RTX 3060 Ti ä¸­æ–‡è§†é¢‘è½¬å­—å¹•å·¥å…·
æ”¯æŒå¤šç§æ¨¡å‹ï¼šWhisperã€Faster-Whisperã€FunASR
é’ˆå¯¹ä¸­æ–‡ç”µè§†å‰§ä¼˜åŒ–ï¼Œæ”¯æŒTensorRTåŠ é€Ÿ
"""

import os
import sys
import logging
import argparse
import time
import json
from pathlib import Path
from typing import Optional, Dict, List, Any
from tqdm import tqdm
import subprocess

# è®¾ç½®ç¯å¢ƒå˜é‡
os.environ['CUDA_LAZY_LOADING'] = '1'
os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.StreamHandler(sys.stdout),
        logging.FileHandler('conversion.log', encoding='utf-8')
    ]
)
logger = logging.getLogger(__name__)

# è®¾ç½®FFmpegè·¯å¾„
ffmpeg_path = r"D:\code\ffmpeg\bin"
if os.path.exists(ffmpeg_path):
    os.environ["PATH"] += os.pathsep + ffmpeg_path

# æ¨¡å‹é…ç½®
SUPPORTED_MODELS = {
    # Faster-Whisper æ¨¡å‹
    "faster-tiny": {"size": "39MB", "model_id": "guillaumekln/faster-whisper-tiny"},
    "faster-base": {"size": "142MB", "model_id": "guillaumekln/faster-whisper-base"},
    "faster-small": {"size": "461MB", "model_id": "guillaumekln/faster-whisper-small"},
    "faster-medium": {"size": "1.5GB", "model_id": "guillaumekln/faster-whisper-medium"},
    "faster-large": {"size": "2.9GB", "model_id": "guillaumekln/faster-whisper-large-v2"},
    "faster-large-v3": {"size": "2.9GB", "model_id": "guillaumekln/faster-whisper-large-v3"},
    
    # æ ‡å‡† Whisper æ¨¡å‹
    "tiny": {"size": "39MB", "model_id": "tiny"},
    "base": {"size": "142MB", "model_id": "base"},
    "small": {"size": "461MB", "model_id": "small"},
    "medium": {"size": "1.5GB", "model_id": "medium"},
    "large": {"size": "2.9GB", "model_id": "large"},
    "large-v2": {"size": "2.9GB", "model_id": "large-v2"},
    "large-v3": {"size": "2.9GB", "model_id": "large-v3"},
    
    # ä¸­æ–‡ä¼˜åŒ–æ¨¡å‹
    "chinese-whisper-small": {"size": "461MB", "model_id": "openai/whisper-small"},
    "chinese-whisper-base": {"size": "142MB", "model_id": "openai/whisper-base"},
}

# æ£€æŸ¥ä¾èµ–
try:
    import torch
    import torchaudio
    logger.info(f"PyTorchç‰ˆæœ¬: {torch.__version__}")
    if torch.cuda.is_available():
        logger.info(f"CUDAç‰ˆæœ¬: {torch.version.cuda}")
        logger.info(f"GPU: {torch.cuda.get_device_name(0)}")
    else:
        logger.warning("CUDAä¸å¯ç”¨ï¼Œå°†ä½¿ç”¨CPUæ¨¡å¼")
except ImportError:
    logger.error("PyTorchæœªå®‰è£…ï¼è¯·è¿è¡Œ: pip install torch torchaudio --index-url https://download.pytorch.org/whl/cu121")
    sys.exit(1)

try:
    from faster_whisper import WhisperModel as FasterWhisperModel
    FASTER_WHISPER_AVAILABLE = True
except ImportError:
    FASTER_WHISPER_AVAILABLE = False
    logger.warning("Faster-Whisperæœªå®‰è£…")

try:
    import whisper
    WHISPER_AVAILABLE = True
except ImportError:
    WHISPER_AVAILABLE = False
    logger.warning("OpenAI Whisperæœªå®‰è£…")

try:
    import moviepy.editor as mp
    MOVIEPY_AVAILABLE = True
except ImportError:
    MOVIEPY_AVAILABLE = False
    logger.warning("MoviePyæœªå®‰è£…")

try:
    import jieba
    JIEBA_AVAILABLE = True
except ImportError:
    JIEBA_AVAILABLE = False
    logger.warning("Jiebaæœªå®‰è£…ï¼Œä¸­æ–‡åˆ†è¯åŠŸèƒ½ä¸å¯ç”¨")

try:
    from huggingface_hub import hf_hub_download
    from huggingface_hub.utils import HfHubHTTPError
    HF_HUB_AVAILABLE = True
except ImportError:
    HF_HUB_AVAILABLE = False
    logger.warning("Hugging Face Hubæœªå®‰è£…")


class Config:
    """é…ç½®ç±»"""
    def __init__(self, **kwargs):
        # åŸºç¡€é…ç½®
        self.rtx_3060_ti_optimized = kwargs.get('rtx_3060_ti_optimized', True)
        self.max_memory_usage = kwargs.get('max_memory_usage', 0.8)
        self.batch_size = kwargs.get('batch_size', 16)
        self.chunk_length = kwargs.get('chunk_length', 30)
        self.device = kwargs.get('device', "cuda" if torch.cuda.is_available() else "cpu")
        
        # éŸ³é¢‘å¤„ç†é…ç½®
        self.audio_quality = kwargs.get('audio_quality', 'balanced')
        self.enable_audio_preprocessing = kwargs.get('enable_audio_preprocessing', True)
        self.sample_rate = kwargs.get('sample_rate', 16000)
        
        # æ¨¡å‹é…ç½®
        self.model_cache_dir = kwargs.get('model_cache_dir', './models')
        self.compute_type = kwargs.get('compute_type', 'auto')
        self.beam_size = kwargs.get('beam_size', 5)
        self.best_of = kwargs.get('best_of', 5)
        self.temperature = kwargs.get('temperature', 0.0)
        
        # æ–‡æœ¬å¤„ç†é…ç½®
        self.enable_text_correction = kwargs.get('enable_text_correction', True)
        self.language = kwargs.get('language', 'zh')
        self.verbose = kwargs.get('verbose', False)
        
        # è¾“å‡ºé…ç½®
        self.output_format = kwargs.get('output_format', 'srt')
        self.keep_temp = kwargs.get('keep_temp', False)
        
        # RTX 3060 Ti ç‰¹å®šä¼˜åŒ–
        if torch.cuda.is_available():
            gpu_name = torch.cuda.get_device_name(0)
            if "3060 Ti" in gpu_name:
                self.batch_size = kwargs.get('batch_size', 8)
                self.chunk_length = kwargs.get('chunk_length', 20)
                logger.info("æ£€æµ‹åˆ°RTX 3060 Tiï¼Œå·²åº”ç”¨ä¼˜åŒ–è®¾ç½®")
        
        # åˆ›å»ºæ¨¡å‹ç›®å½•
        os.makedirs(self.model_cache_dir, exist_ok=True)


class ModelDownloader:
    """æ¨¡å‹ä¸‹è½½å™¨ï¼Œå¸¦è¿›åº¦æ˜¾ç¤º"""
    
    def __init__(self, cache_dir: str = "./models"):
        self.cache_dir = Path(cache_dir)
        self.cache_dir.mkdir(exist_ok=True)
    
    def download_with_progress(self, model_name: str, model_info: Dict) -> str:
        """ä¸‹è½½æ¨¡å‹å¹¶æ˜¾ç¤ºè¿›åº¦"""
        if model_name not in SUPPORTED_MODELS:
            raise ValueError(f"ä¸æ”¯æŒçš„æ¨¡å‹: {model_name}")
        
        model_path = self.cache_dir / model_name
        
        # æ£€æŸ¥æ˜¯å¦å·²ç»ä¸‹è½½
        if model_path.exists() and self._is_model_complete(model_path):
            logger.info(f"æ¨¡å‹ {model_name} å·²å­˜åœ¨ï¼Œè·³è¿‡ä¸‹è½½")
            return str(model_path)
        
        logger.info(f"æ­£åœ¨ä¸‹è½½ {model_name} ({model_info['size']})...")
        
        try:
            if model_name.startswith("faster-"):
                return self._download_faster_whisper(model_name, model_info)
            else:
                return self._download_whisper(model_name, model_info)
        except Exception as e:
            logger.error(f"ä¸‹è½½å¤±è´¥: {e}")
            if model_path.exists():
                import shutil
                shutil.rmtree(model_path)
            raise
    
    def _download_faster_whisper(self, model_name: str, model_info: Dict) -> str:
        """ä¸‹è½½ Faster-Whisper æ¨¡å‹"""
        if not FASTER_WHISPER_AVAILABLE:
            raise ImportError("Faster-Whisper æœªå®‰è£…")
        
        try:
            # å¯¹äº faster-whisperï¼Œæˆ‘ä»¬ç›´æ¥ä½¿ç”¨æ¨¡å‹ ID
            model_id = model_info["model_id"]
            
            # åˆ›å»ºè¿›åº¦æ¡
            with tqdm(desc=f"ä¸‹è½½ {model_name}", unit="B", unit_scale=True) as pbar:
                # åˆå§‹åŒ–æ¨¡å‹ï¼ˆè¿™ä¼šè§¦å‘ä¸‹è½½ï¼‰
                model = FasterWhisperModel(
                    model_id,
                    device="cpu",  # å…ˆç”¨CPUåˆå§‹åŒ–
                    compute_type="int8",
                    download_root=str(self.cache_dir)
                )
                
                # æ›´æ–°è¿›åº¦æ¡
                pbar.set_description(f"ä¸‹è½½å®Œæˆ: {model_name}")
                pbar.update(100)
            
            logger.info(f"æ¨¡å‹ {model_name} ä¸‹è½½å®Œæˆ")
            return model_id
            
        except Exception as e:
            logger.error(f"Faster-Whisper æ¨¡å‹ä¸‹è½½å¤±è´¥: {e}")
            raise
    
    def _download_whisper(self, model_name: str, model_info: Dict) -> str:
        """ä¸‹è½½æ ‡å‡† Whisper æ¨¡å‹"""
        if not WHISPER_AVAILABLE:
            raise ImportError("OpenAI Whisper æœªå®‰è£…")
        
        try:
            with tqdm(desc=f"ä¸‹è½½ {model_name}", unit="B", unit_scale=True) as pbar:
                # ä½¿ç”¨ whisper.load_model ä¼šè‡ªåŠ¨ä¸‹è½½
                model = whisper.load_model(model_info["model_id"])
                pbar.set_description(f"ä¸‹è½½å®Œæˆ: {model_name}")
                pbar.update(100)
            
            logger.info(f"æ¨¡å‹ {model_name} ä¸‹è½½å®Œæˆ")
            return model_info["model_id"]
            
        except Exception as e:
            logger.error(f"Whisper æ¨¡å‹ä¸‹è½½å¤±è´¥: {e}")
            raise
    
    def _is_model_complete(self, model_path: Path) -> bool:
        """æ£€æŸ¥æ¨¡å‹æ˜¯å¦å®Œæ•´"""
        return model_path.exists() and any(model_path.iterdir())


class AudioProcessor:
    """éŸ³é¢‘å¤„ç†å™¨"""
    
    def __init__(self, config: Config = None):
        self.config = config or Config()

    def extract_audio(self, video_path: str, output_path: str = None) -> str:
        """ä»è§†é¢‘æå–éŸ³é¢‘"""
        if not MOVIEPY_AVAILABLE:
            raise ImportError("MoviePyæœªå®‰è£…ï¼Œæ— æ³•å¤„ç†è§†é¢‘æ–‡ä»¶")

        if output_path is None:
            output_path = video_path.rsplit('.', 1)[0] + '_audio.wav'

        logger.info(f"æ­£åœ¨æå–éŸ³é¢‘: {video_path}")

        try:
            with tqdm(desc="æå–éŸ³é¢‘", unit="s") as pbar:
                video = mp.VideoFileClip(video_path)
                audio = video.audio
                
                # è®¾ç½®è¿›åº¦å›è°ƒ
                def progress_callback(t):
                    pbar.n = int(t)
                    pbar.refresh()
                
                audio.write_audiofile(
                    output_path, 
                    verbose=False, 
                    logger=None,
                    progress_bar=False
                )
                
                pbar.update(int(video.duration))
                video.close()
                audio.close()

            logger.info(f"éŸ³é¢‘æå–å®Œæˆ: {output_path}")
            return output_path

        except Exception as e:
            logger.error(f"éŸ³é¢‘æå–å¤±è´¥: {e}")
            raise


class TextProcessor:
    """æ–‡æœ¬åå¤„ç†å™¨"""

    def __init__(self, config: Config = None):
        self.config = config or Config()
        
        # å¸¸è§é”™åˆ«å­—è¯å…¸
        self.corrections = {
            "çº³é‡Œ": "é‚£é‡Œ",
            "äº‹å": "æ—¶å€™", 
            "åªèƒ½": "æ™ºèƒ½",
            "é©¬ä¸Š": "é©¬ä¸Š",
            "å› è¯¥": "åº”è¯¥",
            "çš„è¯": "çš„è¯",
            "åœ¨è¿™ä¸ª": "åœ¨è¿™ä¸ª",
            "ç„¶å": "ç„¶å",
            "è¿™ä¸ª": "è¿™ä¸ª",
            "é‚£ä¸ª": "é‚£ä¸ª",
            "ä»€ä¹ˆ": "ä»€ä¹ˆ",
            "æ€ä¹ˆ": "æ€ä¹ˆ",
            "ä¸ºä»€ä¹ˆ": "ä¸ºä»€ä¹ˆ",
            "ä½†æ˜¯": "ä½†æ˜¯",
            "æ‰€ä»¥": "æ‰€ä»¥",
            "å› ä¸º": "å› ä¸º",
            "å¦‚æœ": "å¦‚æœ",
            "è™½ç„¶": "è™½ç„¶",
            "ä¸è¿‡": "ä¸è¿‡",
            "å¯æ˜¯": "å¯æ˜¯",
            "åªæ˜¯": "åªæ˜¯",
            "è€Œä¸”": "è€Œä¸”",
            "å¹¶ä¸”": "å¹¶ä¸”",
            "æˆ–è€…": "æˆ–è€…",
            "è¿˜æ˜¯": "è¿˜æ˜¯",
            "æ¯”å¦‚": "æ¯”å¦‚",
            "ä¾‹å¦‚": "ä¾‹å¦‚",
            "å°±æ˜¯": "å°±æ˜¯",
            "ä¹Ÿå°±æ˜¯": "ä¹Ÿå°±æ˜¯"
        }

        # ä¸“ä¸šè¯æ±‡
        self.professional_terms = {
            "äººå·¥åªèƒ½": "äººå·¥æ™ºèƒ½",
            "æœºå™¨å­¦ä¹ ": "æœºå™¨å­¦ä¹ ",
            "æ·±åº¦å­¦ç³»": "æ·±åº¦å­¦ä¹ ",
            "ç¥ç»ç½‘ç»œ": "ç¥ç»ç½‘ç»œ",
            "ç®—æ³•": "ç®—æ³•",
            "æ•°æ®": "æ•°æ®",
            "æ¨¡å‹": "æ¨¡å‹",
            "è®­ç»ƒ": "è®­ç»ƒ",
            "ä¼˜åŒ–": "ä¼˜åŒ–",
            "é¢„æµ‹": "é¢„æµ‹",
            "åˆ†æ": "åˆ†æ",
            "å¤„ç†": "å¤„ç†",
            "ç³»ç»Ÿ": "ç³»ç»Ÿ",
            "å¹³å°": "å¹³å°",
            "æŠ€æœ¯": "æŠ€æœ¯",
            "æ–¹æ³•": "æ–¹æ³•",
            "å·¥å…·": "å·¥å…·",
            "è½¯ä»¶": "è½¯ä»¶",
            "ç¡¬ä»¶": "ç¡¬ä»¶",
            "ç½‘ç»œ": "ç½‘ç»œ",
            "äº’è”ç½‘": "äº’è”ç½‘",
            "è®¡ç®—æœº": "è®¡ç®—æœº",
            "ç¨‹åº": "ç¨‹åº",
            "ä»£ç ": "ä»£ç ",
            "å¼€å‘": "å¼€å‘",
            "è®¾è®¡": "è®¾è®¡",
            "åº”ç”¨": "åº”ç”¨",
            "æœåŠ¡": "æœåŠ¡",
            "äº§å“": "äº§å“",
            "é¡¹ç›®": "é¡¹ç›®",
            "ç®¡ç†": "ç®¡ç†",
            "è¿è¥": "è¿è¥",
            "å¸‚åœº": "å¸‚åœº",
            "è¥é”€": "è¥é”€",
            "é”€å”®": "é”€å”®",
            "å®¢æˆ·": "å®¢æˆ·",
            "ç”¨æˆ·": "ç”¨æˆ·",
            "ä½“éªŒ": "ä½“éªŒ",
            "ç•Œé¢": "ç•Œé¢",
            "åŠŸèƒ½": "åŠŸèƒ½",
            "æ€§èƒ½": "æ€§èƒ½",
            "æ•ˆç‡": "æ•ˆç‡",
            "è´¨é‡": "è´¨é‡",
            "å®‰å…¨": "å®‰å…¨",
            "ç¨³å®š": "ç¨³å®š",
            "å¯é ": "å¯é ",
            "åˆ›æ–°": "åˆ›æ–°",
            "å‘å±•": "å‘å±•",
            "è¿›æ­¥": "è¿›æ­¥",
            "æ”¹è¿›": "æ”¹è¿›",
            "å®Œå–„": "å®Œå–„",
            "æå‡": "æå‡",
            "å¢å¼º": "å¢å¼º",
            "æ‰©å±•": "æ‰©å±•",
            "å‡çº§": "å‡çº§",
            "æ›´æ–°": "æ›´æ–°",
            "ç»´æŠ¤": "ç»´æŠ¤",
            "æ”¯æŒ": "æ”¯æŒ",
            "å¸®åŠ©": "å¸®åŠ©",
            "è§£å†³": "è§£å†³",
            "é—®é¢˜": "é—®é¢˜",
            "å›°éš¾": "å›°éš¾",
            "æŒ‘æˆ˜": "æŒ‘æˆ˜",
            "æœºä¼š": "æœºä¼š",
            "ä¼˜åŠ¿": "ä¼˜åŠ¿",
            "ç‰¹ç‚¹": "ç‰¹ç‚¹",
            "ç‰¹è‰²": "ç‰¹è‰²",
            "äº®ç‚¹": "äº®ç‚¹",
            "é‡ç‚¹": "é‡ç‚¹",
            "å…³é”®": "å…³é”®",
            "æ ¸å¿ƒ": "æ ¸å¿ƒ",
            "é‡è¦": "é‡è¦",
            "å¿…è¦": "å¿…è¦",
            "åŸºæœ¬": "åŸºæœ¬",
            "ä¸»è¦": "ä¸»è¦",
            "é¦–è¦": "é¦–è¦",
            "ä¼˜å…ˆ": "ä¼˜å…ˆ",
            "ç´§æ€¥": "ç´§æ€¥",
            "åŠæ—¶": "åŠæ—¶",
            "å¿«é€Ÿ": "å¿«é€Ÿ",
            "é«˜æ•ˆ": "é«˜æ•ˆ",
            "ä¸“ä¸š": "ä¸“ä¸š",
            "ç²¾å‡†": "ç²¾å‡†",
            "å‡†ç¡®": "å‡†ç¡®",
            "æ­£ç¡®": "æ­£ç¡®",
            "åˆç†": "åˆç†",
            "ç§‘å­¦": "ç§‘å­¦",
            "ç³»ç»Ÿ": "ç³»ç»Ÿ",
            "å…¨é¢": "å…¨é¢",
            "å®Œæ•´": "å®Œæ•´",
            "è¯¦ç»†": "è¯¦ç»†",
            "å…·ä½“": "å…·ä½“",
            "æ˜ç¡®": "æ˜ç¡®",
            "æ¸…æ¥š": "æ¸…æ¥š",
            "ç®€å•": "ç®€å•",
            "å¤æ‚": "å¤æ‚",
            "å›°éš¾": "å›°éš¾",
            "å®¹æ˜“": "å®¹æ˜“",
            "æ–¹ä¾¿": "æ–¹ä¾¿",
            "å®ç”¨": "å®ç”¨",
            "æœ‰ç”¨": "æœ‰ç”¨",
            "å¥½ç”¨": "å¥½ç”¨",
            "æ˜“ç”¨": "æ˜“ç”¨"
        }

        if JIEBA_AVAILABLE:
            # æ·»åŠ ä¸“ä¸šè¯æ±‡åˆ°jiebaè¯å…¸
            for term in self.professional_terms.values():
                jieba.add_word(term)

    def correct_text(self, text: str) -> str:
        """çº æ­£æ–‡æœ¬é”™è¯¯"""
        if not self.config.enable_text_correction:
            return text
        
        # åŸºæœ¬çº é”™
        for wrong, correct in self.corrections.items():
            text = text.replace(wrong, correct)

        # ä¸“ä¸šè¯æ±‡çº é”™
        for wrong, correct in self.professional_terms.items():
            text = text.replace(wrong, correct)

        # å»é™¤é‡å¤è¯
        text = self._remove_repetition(text)

        # æ ‡ç‚¹ç¬¦å·ä¼˜åŒ–
        text = self._fix_punctuation(text)

        return text.strip()

    def _remove_repetition(self, text: str) -> str:
        """å»é™¤é‡å¤è¯æ±‡"""
        import re
        # å»é™¤é‡å¤çš„"å—¯"ã€"å•Š"ç­‰è¯­æ°”è¯
        text = re.sub(r'(å—¯){2,}', 'å—¯', text)
        text = re.sub(r'(å•Š){2,}', 'å•Š', text)
        text = re.sub(r'(é‚£ä¸ª){2,}', 'é‚£ä¸ª', text)
        text = re.sub(r'(è¿™ä¸ª){2,}', 'è¿™ä¸ª', text)
        text = re.sub(r'(å°±æ˜¯){2,}', 'å°±æ˜¯', text)
        text = re.sub(r'(ç„¶å){2,}', 'ç„¶å', text)
        return text

    def _fix_punctuation(self, text: str) -> str:
        """ä¿®å¤æ ‡ç‚¹ç¬¦å·"""
        import re
        # å¥æœ«æ·»åŠ æ ‡ç‚¹
        if text and not text[-1] in 'ã€‚ï¼ï¼Ÿï¼Œï¼›':
            if 'ï¼Ÿ' in text or 'ä»€ä¹ˆ' in text or 'æ€ä¹ˆ' in text or 'ä¸ºä»€ä¹ˆ' in text:
                text += 'ï¼Ÿ'
            elif 'ï¼' in text or text.endswith(('å•Š', 'å‘€', 'å“‡', 'å“', 'å”‰')):
                text += 'ï¼'
            else:
                text += 'ã€‚'
        return text


class WhisperModel:
    """Whisperæ¨¡å‹åŒ…è£…å™¨"""

    def __init__(self, model_name: str = "base", device: str = "cuda", config: Config = None):
        self.model_name = model_name
        self.device = device if torch.cuda.is_available() else "cpu"
        self.model = None
        self.config = config or Config()
        self.text_processor = TextProcessor(config)
        self.model_downloader = ModelDownloader(self.config.model_cache_dir)
        
        # éªŒè¯æ¨¡å‹åç§°
        if model_name not in SUPPORTED_MODELS:
            available_models = ", ".join(SUPPORTED_MODELS.keys())
            raise ValueError(f"ä¸æ”¯æŒçš„æ¨¡å‹: {model_name}ã€‚æ”¯æŒçš„æ¨¡å‹: {available_models}")

    def load_model(self):
        """åŠ è½½æ¨¡å‹"""
        logger.info(f"æ­£åœ¨åŠ è½½æ¨¡å‹: {self.model_name}")
        
        try:
            # ä¸‹è½½æ¨¡å‹
            model_path = self.model_downloader.download_with_progress(
                self.model_name, 
                SUPPORTED_MODELS[self.model_name]
            )
            
            # åŠ è½½æ¨¡å‹
            if self.model_name.startswith("faster-") and FASTER_WHISPER_AVAILABLE:
                compute_type = "float16" if self.device == "cuda" else "int8"
                if self.config.compute_type != 'auto':
                    compute_type = self.config.compute_type
                
                self.model = FasterWhisperModel(
                    model_path,
                    device=self.device,
                    compute_type=compute_type,
                    download_root=self.config.model_cache_dir
                )
                logger.info("ä½¿ç”¨Faster-Whisperæ¨¡å‹")
                
            elif WHISPER_AVAILABLE:
                self.model = whisper.load_model(model_path, device=self.device)
                logger.info("ä½¿ç”¨OpenAI Whisperæ¨¡å‹")
            else:
                raise ImportError("æ²¡æœ‰å¯ç”¨çš„Whisperæ¨¡å‹ï¼")
                
        except Exception as e:
            logger.error(f"æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
            raise

    def transcribe(self, audio_path: str) -> List[Dict]:
        """è½¬å½•éŸ³é¢‘"""
        if self.model is None:
            self.load_model()

        logger.info(f"å¼€å§‹è½¬å½•: {audio_path}")
        start_time = time.time()

        try:
            if self.model_name.startswith("faster-") and FASTER_WHISPER_AVAILABLE:
                # Faster-Whisper
                with tqdm(desc="è½¬å½•è¿›åº¦", unit="ç§’") as pbar:
                    segments, info = self.model.transcribe(
                        audio_path,
                        language=self.config.language,
                        beam_size=self.config.beam_size,
                        best_of=self.config.best_of,
                        temperature=self.config.temperature,
                        progress_callback=lambda x: pbar.update(1)
                    )

                    results = []
                    for segment in segments:
                        text = self.text_processor.correct_text(segment.text)
                        results.append({
                            'start': segment.start,
                            'end': segment.end,
                            'text': text
                        })
                        pbar.update(1)

            else:
                # OpenAI Whisper
                with tqdm(desc="è½¬å½•è¿›åº¦", unit="ç§’") as pbar:
                    result = self.model.transcribe(
                        audio_path, 
                        language=self.config.language,
                        verbose=self.config.verbose
                    )
                    
                    results = []
                    for segment in result['segments']:
                        text = self.text_processor.correct_text(segment['text'])
                        results.append({
                            'start': segment['start'],
                            'end': segment['end'],
                            'text': text
                        })
                        pbar.update(1)

            duration = time.time() - start_time
            logger.info(f"è½¬å½•å®Œæˆï¼Œè€—æ—¶: {duration:.2f}ç§’")
            return results

        except Exception as e:
            logger.error(f"è½¬å½•å¤±è´¥: {e}")
            raise


class SRTGenerator:
    """SRTå­—å¹•ç”Ÿæˆå™¨"""

    @staticmethod
    def format_timestamp(seconds: float) -> str:
        """æ ¼å¼åŒ–æ—¶é—´æˆ³"""
        hours = int(seconds // 3600)
        minutes = int((seconds % 3600) // 60)
        secs = int(seconds % 60)
        millisecs = int((seconds % 1) * 1000)
        return f"{hours:02d}:{minutes:02d}:{secs:02d},{millisecs:03d}"

    @staticmethod
    def generate_srt(segments: List[Dict], output_path: str):
        """ç”ŸæˆSRTæ–‡ä»¶"""
        logger.info(f"æ­£åœ¨ç”ŸæˆSRTæ–‡ä»¶: {output_path}")

        with open(output_path, 'w', encoding='utf-8') as f:
            for i, segment in enumerate(segments, 1):
                start_time = SRTGenerator.format_timestamp(segment['start'])
                end_time = SRTGenerator.format_timestamp(segment['end'])

                f.write(f"{i}\n")
                f.write(f"{start_time} --> {end_time}\n")
                f.write(f"{segment['text']}\n\n")

        logger.info(f"SRTæ–‡ä»¶ç”Ÿæˆå®Œæˆ: {output_path}")


def print_supported_models():
    """æ‰“å°æ”¯æŒçš„æ¨¡å‹åˆ—è¡¨"""
    print("æ”¯æŒçš„æ¨¡å‹åˆ—è¡¨:")
    print("=" * 60)
    
    print("\nğŸš€ Faster-Whisper æ¨¡å‹ (æ¨è):")
    for model, info in SUPPORTED_MODELS.items():
        if model.startswith("faster-"):
            print(f"  {model:<20} - {info['size']}")
    
    print("\nğŸ“¦ æ ‡å‡† Whisper æ¨¡å‹:")
    for model, info in SUPPORTED_MODELS.items():
        if not model.startswith("faster-") and not model.startswith("chinese-"):
            print(f"  {model:<20} - {info['size']}")
    
    print("\nğŸ‡¨ğŸ‡³ ä¸­æ–‡ä¼˜åŒ–æ¨¡å‹:")
    for model, info in SUPPORTED_MODELS.items():
        if model.startswith("chinese-"):
            print(f"  {model:<20} - {info['size']}")
    
    print("\nğŸ’¡ RTX 3060 Ti æ¨è:")
    print("  - faster-base     (å¹³è¡¡æ€§èƒ½å’Œè´¨é‡)")
    print("  - faster-small    (å¿«é€Ÿå¤„ç†)")
    print("  - base            (æ ‡å‡†é€‰æ‹©)")
    print("  âš ï¸  é¿å…ä½¿ç”¨ medium/large (æ˜¾å­˜å¯èƒ½ä¸è¶³)")


def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(
        description="RTX 3060 Ti è§†é¢‘è½¬å­—å¹•å·¥å…·",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="ç¤ºä¾‹:\n"
               "  python main.py video.mp4\n"
               "  python main.py video.mp4 --model faster-base\n"
               "  python main.py video.mp4 --model faster-base --output subtitle.srt\n"
               "  python main.py video.mp4 --audio-quality high --enable-text-correction\n"
               "  python main.py --list-models"
    )
    
    parser.add_argument("video_path", nargs='?', help="è§†é¢‘æ–‡ä»¶è·¯å¾„")
    parser.add_argument("--model", default="base", 
                       help=f"é€‰æ‹©æ¨¡å‹ (é»˜è®¤: base)")
    parser.add_argument("--output", default=None, help="è¾“å‡ºSRTæ–‡ä»¶è·¯å¾„")
    parser.add_argument("--device", default="auto", 
                       choices=["auto", "cuda", "cpu"], help="è®¡ç®—è®¾å¤‡ (é»˜è®¤: auto)")
    parser.add_argument("--language", default="zh", help="è¯­è¨€ä»£ç  (é»˜è®¤: zh)")
    parser.add_argument("--audio-quality", default="balanced",
                       choices=["fast", "balanced", "high"], 
                       help="éŸ³é¢‘è´¨é‡ (é»˜è®¤: balanced)")
    parser.add_argument("--batch-size", type=int, default=8, help="æ‰¹å¤„ç†å¤§å° (é»˜è®¤: 8)")
    parser.add_argument("--beam-size", type=int, default=5, help="æŸæœç´¢å¤§å° (é»˜è®¤: 5)")
    parser.add_argument("--temperature", type=float, default=0.0, help="æ¸©åº¦å‚æ•° (é»˜è®¤: 0.0)")
    parser.add_argument("--compute-type", default="auto", help="è®¡ç®—ç±»å‹ (é»˜è®¤: auto)")
    parser.add_argument("--chunk-length", type=int, default=20, help="éŸ³é¢‘å—é•¿åº¦ (é»˜è®¤: 20)")
    parser.add_argument("--enable-text-correction", action="store_true", default=True,
                       help="å¯ç”¨æ–‡æœ¬çº é”™ (é»˜è®¤: True)")
    parser.add_argument("--enable-audio-preprocessing", action="store_true", default=True,
                       help="å¯ç”¨éŸ³é¢‘é¢„å¤„ç† (é»˜è®¤: True)")
    parser.add_argument("--keep-temp", action="store_true", default=False,
                       help="ä¿ç•™ä¸´æ—¶æ–‡ä»¶ (é»˜è®¤: False)")
    parser.add_argument("--verbose", action="store_true", default=False,
                       help="è¯¦ç»†è¾“å‡º (é»˜è®¤: False)")
    parser.add_argument("--list-models", action="store_true", help="åˆ—å‡ºæ”¯æŒçš„æ¨¡å‹")
    
    args = parser.parse_args()
    
    # åˆ—å‡ºæ”¯æŒçš„æ¨¡å‹
    if args.list_models:
        print_supported_models()
        return
    
    # æ£€æŸ¥è¾“å…¥æ–‡ä»¶
    if not args.video_path:
        parser.error("è¯·æŒ‡å®šè§†é¢‘æ–‡ä»¶è·¯å¾„")
    
    if not os.path.exists(args.video_path):
        logger.error(f"æ–‡ä»¶ä¸å­˜åœ¨: {args.video_path}")
        return

    # éªŒè¯æ¨¡å‹åç§°
    if args.model not in SUPPORTED_MODELS:
        logger.error(f"ä¸æ”¯æŒçš„æ¨¡å‹: {args.model}")
        print_supported_models()
        return

    # è®¾ç½®è¾“å‡ºè·¯å¾„
    if args.output is None:
        args.output = args.video_path.rsplit('.', 1)[0] + '.srt'

    # è®¾ç½®è®¾å¤‡
    if args.device == "auto":
        device = "cuda" if torch.cuda.is_available() else "cpu"
    else:
        device = args.device

    # åˆ›å»ºé…ç½®
    config = Config(
        device=device,
        audio_quality=args.audio_quality,
        batch_size=args.batch_size,
        beam_size=args.beam_size,
        temperature=args.temperature,
        compute_type=args.compute_type,
        chunk_length=args.chunk_length,
        enable_text_correction=args.enable_text_correction,
        enable_audio_preprocessing=args.enable_audio_preprocessing,
        keep_temp=args.keep_temp,
        verbose=args.verbose,
        language=args.language
    )

    try:
        # æå–éŸ³é¢‘
        audio_processor = AudioProcessor(config)
        audio_path = audio_processor.extract_audio(args.video_path)

        # è½¬å½•éŸ³é¢‘
        model = WhisperModel(args.model, device, config)
        segments = model.transcribe(audio_path)

        # ç”Ÿæˆå­—å¹•
        SRTGenerator.generate_srt(segments, args.output)

        # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
        if not config.keep_temp and os.path.exists(audio_path) and audio_path != args.video_path:
            os.remove(audio_path)

        logger.info("è½¬æ¢å®Œæˆï¼")

    except Exception as e:
        logger.error(f"è½¬æ¢å¤±è´¥: {e}")
        raise


if __name__ == "__main__":
    main()
