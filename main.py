import os
import sys
import time
import torch
import argparse
import traceback
import subprocess
import platform
from datetime import timedelta
from typing import List, Dict, Any, Optional
import logging
import re
import numpy as np
import soundfile as sf
import gc
from tqdm import tqdm
import psutil
import json
from text_postprocessor import TextPostProcessor

# å¯¼å…¥TensorRTç®¡ç†å™¨
try:
    from tensorrt_manager import TensorRTEngineManager
    TENSORRT_MANAGER_AVAILABLE = True
    logger.info("TensorRT Managerå¯¼å…¥æˆåŠŸ")
except ImportError as e:
    TENSORRT_MANAGER_AVAILABLE = False
    logger.warning(f"TensorRT Managerä¸å¯ç”¨: {e}")
    logger.info("å°†ä½¿ç”¨æ ‡å‡†æ¨¡å¼è¿è¡Œ")

# é…ç½®æ—¥å¿— - ä¿®å¤Windowsç¼–ç é—®é¢˜
import locale
import sys

# è®¾ç½®æ§åˆ¶å°ç¼–ç ä¸ºUTF-8
if sys.platform.startswith('win'):
    try:
        # å°è¯•è®¾ç½®æ§åˆ¶å°ä¸ºUTF-8
        import io
        sys.stdout = io.TextIOWrapper(sys.stdout.buffer, encoding='utf-8')
        sys.stderr = io.TextIOWrapper(sys.stderr.buffer, encoding='utf-8')
    except:
        pass

logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s [%(levelname)s] %(message)s",
    handlers=[
        logging.FileHandler("video_subtitle.log", encoding='utf-8'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

# æ›¿æ¢emojiå­—ç¬¦ä»¥é¿å…ç¼–ç é—®é¢˜
def safe_log_message(message):
    """å®‰å…¨çš„æ—¥å¿—æ¶ˆæ¯ï¼Œæ›¿æ¢å¯èƒ½å¯¼è‡´ç¼–ç é—®é¢˜çš„å­—ç¬¦"""
    emoji_map = {
        'ğŸ¬': '[VIDEO]',
        'ğŸ¤–': '[MODEL]', 
        'ğŸ’»': '[DEVICE]',
        'âœ…': '[OK]',
        'âŒ': '[ERROR]',
        'âš ï¸': '[WARNING]',
        'ğŸš€': '[START]',
        'ğŸ”„': '[LOADING]',
        'ğŸ“Š': '[INFO]',
        'ğŸ§¹': '[CLEANUP]',
        'ğŸ—‘ï¸': '[DELETE]',
        'ğŸ“': '[SAVE]',
        'ğŸ¯': '[TARGET]',
        'ğŸ”': '[CHECK]',
        'âœ¨': '[ENHANCE]',
        'ğŸ‰': '[SUCCESS]'
    }
    for emoji, replacement in emoji_map.items():
        message = message.replace(emoji, replacement)
    return message

# é‡å†™loggeræ–¹æ³•
original_info = logger.info
original_warning = logger.warning
original_error = logger.error

def safe_info(message, *args, **kwargs):
    return original_info(safe_log_message(str(message)), *args, **kwargs)

def safe_warning(message, *args, **kwargs):
    return original_warning(safe_log_message(str(message)), *args, **kwargs)

def safe_error(message, *args, **kwargs):
    return original_error(safe_log_message(str(message)), *args, **kwargs)

logger.info = safe_info
logger.warning = safe_warning
logger.error = safe_error

# è®¾ç½®CUDAç¯å¢ƒå˜é‡ä¼˜åŒ–RTX 3060 Ti
os.environ['CUDA_LAZY_LOADING'] = '1'
os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True,garbage_collection_threshold:0.8'
os.environ['CUDA_VISIBLE_DEVICES'] = '0'

# å°è¯•å¯¼å…¥ä¾èµ–
try:
    import whisper
    from faster_whisper import WhisperModel
    WHISPER_AVAILABLE = True
except ImportError:
    WHISPER_AVAILABLE = False
    logger.warning("Whisperåº“æœªå®‰è£…")

try:
    from transformers import AutoModelForSpeechSeq2Seq, AutoProcessor, pipeline
    HF_AVAILABLE = True
except ImportError:
    HF_AVAILABLE = False
    logger.warning("Transformersåº“æœªå®‰è£…")

try:
    import tensorrt as trt
    import pycuda.driver as cuda
    import pycuda.autoinit
    import onnx
    import onnxruntime as ort
    TENSORRT_AVAILABLE = True
    ONNX_AVAILABLE = True
except ImportError:
    TENSORRT_AVAILABLE = False
    ONNX_AVAILABLE = False
    logger.warning("TensorRT/ONNXä¸å¯ç”¨ï¼Œå°†ä½¿ç”¨PyTorchåŠ é€Ÿ")

try:
    from moviepy.editor import VideoFileClip
    MOVIEPY_AVAILABLE = True
except ImportError:
    MOVIEPY_AVAILABLE = False
    logger.warning("MoviePyæœªå®‰è£…ï¼Œå°†ä½¿ç”¨FFmpegå¤„ç†éŸ³é¢‘")

# æ–°å¢FunASRå¯¼å…¥
try:
    from funasr import AutoModel
    FUNASR_AVAILABLE = True
    logger.info("FunASRåº“å¯¼å…¥æˆåŠŸ")
except ImportError:
    FUNASR_AVAILABLE = False
    AutoModel = None
    logger.warning("æœªæ‰¾åˆ°FunASRåº“ï¼Œè¯·ç¡®ä¿å·²å®‰è£…: pip install funasr")

class Config:
    """é…ç½®ç®¡ç†ç±»"""
    def __init__(self):
        self.config_file = "config.json"
        self.load_config()

    def load_config(self):
        """åŠ è½½é…ç½®æ–‡ä»¶"""
        default_config = {
            "models_path": "./models",
            "temp_path": "./temp",
            "output_path": "./output",
            "gpu_memory_fraction": 0.85,
            "batch_size": 4,
            "max_segment_length": 30,
            "preferred_model": "faster-base",
            "use_tensorrt": True,
            "audio_sample_rate": 16000
        }

        if os.path.exists(self.config_file):
            try:
                with open(self.config_file, 'r', encoding='utf-8') as f:
                    config = json.load(f)
                    # åˆå¹¶é»˜è®¤é…ç½®å’Œç”¨æˆ·é…ç½®
                    for key, value in default_config.items():
                        if key not in config:
                            config[key] = value
                    self.config = config
            except Exception as e:
                logger.warning(f"é…ç½®æ–‡ä»¶åŠ è½½å¤±è´¥ï¼Œä½¿ç”¨é»˜è®¤é…ç½®: {e}")
                self.config = default_config
        else:
            self.config = default_config
            self.save_config()

    def save_config(self):
        """ä¿å­˜é…ç½®æ–‡ä»¶"""
        try:
            with open(self.config_file, 'w', encoding='utf-8') as f:
                json.dump(self.config, f, indent=2, ensure_ascii=False)
        except Exception as e:
            logger.error(f"é…ç½®æ–‡ä»¶ä¿å­˜å¤±è´¥: {e}")

    def get(self, key, default=None):
        return self.config.get(key, default)

    def set(self, key, value):
        self.config[key] = value
        self.save_config()

class Timer:
    """è®¡æ—¶å™¨ç±»"""
    def __init__(self, name="ä»»åŠ¡"):
        self.name = name

    def __enter__(self):
        self.start_time = time.time()
        print(f"ğŸš€ å¼€å§‹ {self.name}...")
        return self

    def __exit__(self, exc_type, exc_val, exc_tb):
        end_time = time.time()
        duration = end_time - self.start_time
        print(f"âœ… {self.name} å®Œæˆï¼Œè€—æ—¶: {duration:.2f} ç§’")

class ProgressTracker:
    """è¿›åº¦è·Ÿè¸ªå™¨"""
    def __init__(self, total_steps=100, description="å¤„ç†ä¸­"):
        self.total_steps = total_steps
        self.current_step = 0
        self.description = description
        self.pbar = tqdm(total=total_steps, desc=description, unit="step")

    def update(self, steps=1, description=None):
        if description:
            self.pbar.set_description(description)
        self.pbar.update(steps)
        self.current_step += steps

    def set_progress(self, current, total=None, description=None):
        if total:
            self.pbar.total = total
        if description:
            self.pbar.set_description(description)
        self.pbar.n = current
        self.pbar.refresh()

    def close(self):
        self.pbar.close()

class RTX3060TiOptimizer:
    """RTX 3060 Tiæ˜¾å¡ä¼˜åŒ–å™¨"""

    @staticmethod
    def setup_gpu_memory(memory_fraction=0.85):
        """é…ç½®GPUæ˜¾å­˜ç®¡ç†"""
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
            try:
                total_memory = torch.cuda.get_device_properties(0).total_memory
                max_memory = int(total_memory * memory_fraction)
                torch.cuda.set_per_process_memory_fraction(memory_fraction)

                # å¯ç”¨å†…å­˜æ± ä¼˜åŒ–
                os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'max_split_size_mb:128,expandable_segments:True'

                logger.info(f"GPUæ˜¾å­˜ä¼˜åŒ–å®Œæˆï¼Œæ€»æ˜¾å­˜: {total_memory/1024**3:.1f}GBï¼Œé¢„ç•™ä½¿ç”¨: {max_memory/1024**3:.1f}GB")
            except Exception as e:
                logger.warning(f"æ˜¾å­˜ä¼˜åŒ–å¤±è´¥: {e}")

    @staticmethod
    def get_optimal_batch_size():
        """è·å–æœ€ä¼˜æ‰¹å¤„ç†å¤§å°"""
        if torch.cuda.is_available():
            total_memory = torch.cuda.get_device_properties(0).total_memory / 1024**3
            if total_memory > 5.5:  # 6GBæ˜¾å¡
                return 4
            else:
                return 2
        return 1

    @staticmethod
    def optimize_cuda_settings():
        """ä¼˜åŒ–CUDAè®¾ç½®"""
        if torch.cuda.is_available():
            # å¯ç”¨TensorRTä¼˜åŒ–
            torch.backends.cudnn.benchmark = True
            torch.backends.cudnn.deterministic = False
            torch.backends.cudnn.enabled = True

            # è®¾ç½®CUDAæµä¼˜åŒ–
            torch.cuda.synchronize()

            # å†…å­˜ä¼˜åŒ–
            torch.cuda.empty_cache()

            logger.info("CUDAä¼˜åŒ–è®¾ç½®å®Œæˆ")

    @staticmethod
    def get_tensorrt_engine_path(model_name: str, config: Config) -> str:
        """è·å–TensorRTå¼•æ“æ–‡ä»¶è·¯å¾„"""
        models_path = config.get('models_path', './models')
        engine_dir = os.path.join(models_path, 'tensorrt_engines')
        os.makedirs(engine_dir, exist_ok=True)
        return os.path.join(engine_dir, f"{model_name.replace('/', '_')}.trt")

class SystemChecker:
    """ç³»ç»Ÿæ£€æŸ¥å™¨"""
    @staticmethod
    def check_cuda():
        """æ£€æŸ¥CUDAç¯å¢ƒ"""
        if not torch.cuda.is_available():
            logger.error("âŒ CUDAä¸å¯ç”¨ï¼è¯·æ£€æŸ¥NVIDIAé©±åŠ¨å’ŒCUDAå®‰è£…")
            return False

        cuda_version = torch.version.cuda
        gpu_name = torch.cuda.get_device_name(0)
        gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1024**3

        logger.info(f"âœ… CUDAç‰ˆæœ¬: {cuda_version}")
        logger.info(f"âœ… GPU: {gpu_name}")
        logger.info(f"âœ… GPUæ˜¾å­˜: {gpu_memory:.2f} GB")

        if "3060 Ti" in gpu_name:
            logger.info("ğŸ¯ æ£€æµ‹åˆ°RTX 3060 Tiï¼Œå·²å¯ç”¨ä¼˜åŒ–é…ç½®")
            RTX3060TiOptimizer.setup_gpu_memory()

        return True

    @staticmethod
    def check_dependencies():
        """æ£€æŸ¥ä¾èµ–æ˜¯å¦å®‰è£…"""
        deps = {
            "torch": True,
            "whisper": WHISPER_AVAILABLE,
            "transformers": HF_AVAILABLE,
            "tensorrt": TENSORRT_AVAILABLE,
            "moviepy": MOVIEPY_AVAILABLE
        }

        missing = [name for name, available in deps.items() if not available]
        if missing:
            logger.warning(f"âš ï¸ å¯é€‰ä¾èµ–ç¼ºå¤±: {', '.join(missing)}")

        required = ["torch", "whisper"]
        missing_required = [name for name in required if not deps.get(name, False)]
        if missing_required:
            logger.error(f"âŒ ç¼ºå°‘å¿…éœ€ä¾èµ–: {', '.join(missing_required)}")
            return False
        return True

class ModelWrapper:
    """æ¨¡å‹åŒ…è£…åŸºç±»"""
    def __init__(self, model_id: str, device: str = "cuda", config: Config = None, **kwargs):
        self.model_id = model_id
        self.device = device
        self.config = config or Config()
        self.kwargs = kwargs
        self.model = None
        self.progress_tracker = None

    def load_model(self):
        raise NotImplementedError

    def transcribe(self, audio_path: str, **kwargs) -> Dict[str, Any]:
        raise NotImplementedError

    def get_gpu_memory_usage(self) -> float:
        """è·å–GPUæ˜¾å­˜ä½¿ç”¨é‡ï¼ˆMBï¼‰"""
        if torch.cuda.is_available():
            return torch.cuda.memory_allocated() / 1024 / 1024
        return 0.0

class WhisperModelWrapper(ModelWrapper):
    """Whisperæ¨¡å‹åŒ…è£…"""
    def load_model(self) -> None:
        """åŠ è½½æ¨¡å‹"""
        try:
            self.progress_tracker = ProgressTracker(100, f"åŠ è½½{self.model_id}æ¨¡å‹")

            if self.device == "cuda" and torch.cuda.is_available():
                RTX3060TiOptimizer.setup_gpu_memory(self.config.get('gpu_memory_fraction', 0.85))

            models_path = self.config.get('models_path', './models')
            os.makedirs(models_path, exist_ok=True)

            self.progress_tracker.update(20, "ä¸‹è½½æ¨¡å‹æ–‡ä»¶...")

            if self.model_id in ["faster-base", "faster-large"]:
                model_mapping = {
                    "faster-base": "base",
                    "faster-large": "large"
                }
                actual_model = model_mapping[self.model_id]
                logger.info(f"ğŸ”„ åŠ è½½Faster-Whisperæ¨¡å‹: {self.model_id} -> {actual_model}")

                self.progress_tracker.update(30, "åˆå§‹åŒ–Faster-Whisper...")
                self.model = WhisperModel(
                    actual_model,
                    device=self.device,
                    compute_type="float16" if self.device == "cuda" else "int8",
                    cpu_threads=4,
                    download_root=models_path
                )
            else:
                logger.info(f"ğŸ”„ åŠ è½½æ ‡å‡†Whisperæ¨¡å‹: {self.model_id}")
                self.progress_tracker.update(30, "åˆå§‹åŒ–Whisper...")
                import whisper
                self.model = whisper.load_model(self.model_id, download_root=models_path)

                if self.device == "cuda":
                    self.model = self.model.cuda()

            self.progress_tracker.update(50, "æ¨¡å‹åŠ è½½å®Œæˆ")
            self.progress_tracker.close()
            logger.info(f"âœ… æ¨¡å‹ {self.model_id} åŠ è½½æˆåŠŸ")

        except Exception as e:
            if self.progress_tracker:
                self.progress_tracker.close()
            logger.error(f"âŒ æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
            raise

    def transcribe(self, audio_path: str, **kwargs) -> Dict[str, Any]:
        """è½¬å½•éŸ³é¢‘"""
        try:
            progress = ProgressTracker(100, "éŸ³é¢‘è½¬å½•ä¸­")

            if self.model_id in ["faster-base", "faster-large"]:
                progress.update(10, "å¼€å§‹Faster-Whisperè½¬å½•...")
                segments, info = self.model.transcribe(
                    audio_path,
                    language="zh",
                    beam_size=1,
                    best_of=1,
                    temperature=0.0,
                    compression_ratio_threshold=2.4,
                    log_prob_threshold=-1.0,
                    no_speech_threshold=0.6,
                    condition_on_previous_text=False,
                    vad_filter=True,
                    vad_parameters=dict(min_silence_duration_ms=500)
                )

                progress.update(60, "å¤„ç†è½¬å½•ç»“æœ...")
                result = {
                    "text": "",
                    "segments": [],
                    "language": info.language
                }

                for i, segment in enumerate(segments):
                    result["segments"].append({
                        "start": segment.start,
                        "end": segment.end,
                        "text": segment.text.strip()
                    })
                    result["text"] += segment.text.strip() + " "
                    if i % 10 == 0:
                        progress.update(2, f"å¤„ç†ç‰‡æ®µ {i+1}")

            else:
                progress.update(10, "å¼€å§‹æ ‡å‡†Whisperè½¬å½•...")
                result = self.model.transcribe(
                    audio_path,
                    language="zh",
                    fp16=torch.cuda.is_available(),
                    verbose=False
                )
                progress.update(80, "è½¬å½•å®Œæˆ")

            progress.close()
            return result

        except Exception as e:
            logger.error(f"âŒ è½¬å½•å¤±è´¥: {e}")
            raise

class TensorRTOptimizer:
    """TensorRTä¼˜åŒ–å™¨"""

    @staticmethod
    def convert_to_tensorrt(onnx_path: str, engine_path: str, precision: str = "fp16") -> bool:
        """å°†ONNXæ¨¡å‹è½¬æ¢ä¸ºTensorRTå¼•æ“"""
        try:
            if not TENSORRT_AVAILABLE:
                logger.warning("TensorRTä¸å¯ç”¨ï¼Œè·³è¿‡ä¼˜åŒ–")
                return False

            if not os.path.exists(onnx_path):
                logger.error(f"ONNXæ–‡ä»¶ä¸å­˜åœ¨: {onnx_path}")
                return False

            logger.info(f"å¼€å§‹è½¬æ¢TensorRTå¼•æ“: {onnx_path} -> {engine_path}")

            # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
            os.makedirs(os.path.dirname(engine_path), exist_ok=True)

            # åˆ›å»ºTensorRT logger
            trt_logger = trt.Logger(trt.Logger.WARNING)

            # åˆ›å»ºbuilderå’Œnetwork
            builder = trt.Builder(trt_logger)
            config = builder.create_builder_config()

            # RTX 3060 Tiä¼˜åŒ–è®¾ç½®
            config.max_workspace_size = 1 << 30  # 1GBï¼ˆæ›´ä¿å®ˆï¼‰
            config.set_flag(trt.BuilderFlag.SPARSE_WEIGHTS) # å¯ç”¨ç¨€ç–æƒé‡ä¼˜åŒ–

            # å¯ç”¨ç²¾åº¦ä¼˜åŒ–
            if precision == "fp16" and builder.platform_has_fast_fp16:
                config.set_flag(trt.BuilderFlag.FP16)
                logger.info("å¯ç”¨FP16ç²¾åº¦ä¼˜åŒ–")
            elif precision == "int8" and builder.platform_has_fast_int8:
                config.set_flag(trt.BuilderFlag.INT8) 
                logger.info("å¯ç”¨INT8ç²¾åº¦ä¼˜åŒ–")

            # åˆ›å»ºç½‘ç»œ
            network = builder.create_network(1 << int(trt.NetworkDefinitionCreationFlag.EXPLICIT_BATCH))

            # è§£æONNXæ–‡ä»¶
            parser = trt.OnnxParser(network, trt_logger)

            logger.info("è§£æONNXæ¨¡å‹...")
            with open(onnx_path, 'rb') as model:
                model_data = model.read()
                if not model_data:
                    logger.error("ONNXæ–‡ä»¶ä¸ºç©º")
                    return False

                if not parser.parse(model_data):
                    logger.error("ONNXè§£æå¤±è´¥ï¼Œé”™è¯¯è¯¦æƒ…:")
                    for error in range(parser.num_errors):
                        logger.error(f"  é”™è¯¯ {error}: {parser.get_error(error)}")
                    return False

            logger.info("ONNXè§£ææˆåŠŸï¼Œå¼€å§‹æ„å»ºå¼•æ“...")

            # æ„å»ºå¼•æ“ï¼ˆæ·»åŠ è¿›åº¦æç¤ºï¼‰
            serialized_engine = builder.build_serialized_network(network, config)
            if serialized_engine is None:
                logger.error("TensorRTå¼•æ“æ„å»ºå¤±è´¥")
                return False

            # ä¿å­˜å¼•æ“
            logger.info("ä¿å­˜TensorRTå¼•æ“...")
            with open(engine_path, 'wb') as f:
                f.write(serialized_engine)

            # éªŒè¯ä¿å­˜çš„æ–‡ä»¶
            if os.path.exists(engine_path):
                file_size = os.path.getsize(engine_path)
                logger.info(f"TensorRTå¼•æ“æ„å»ºæˆåŠŸ: {engine_path} ({file_size/1024/1024:.1f}MB)")
                return True
            else:
                logger.error("å¼•æ“æ–‡ä»¶ä¿å­˜å¤±è´¥")
                return False

        except Exception as e:
            logger.error(f"TensorRTè½¬æ¢å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            return False

    @staticmethod 
    def create_fallback_engine(engine_path: str, model_info: Dict) -> bool:
        """åˆ›å»ºåå¤‡å¼•æ“å‚æ•°æ–‡ä»¶"""
        try:
            logger.info("åˆ›å»ºTensorRTåå¤‡å‚æ•°æ–‡ä»¶...")

            # åˆ›å»ºåŸºæœ¬çš„å¼•æ“é…ç½®æ–‡ä»¶
            fallback_config = {
                "engine_info": {
                    "precision": "fp16",
                    "max_batch_size": 1,
                    "max_workspace_size": 1073741824,  # 1GB
                    "input_shapes": {
                        "audio_input": [-1, 80, -1]  # åŠ¨æ€å½¢çŠ¶
                    },
                    "output_shapes": {
                        "text_output": [-1, -1]
                    }
                },
                "optimization_flags": [
                    "FP16", "SPARSE_WEIGHTS"
                ],
                "created_time": time.time(),
                "rtx_3060ti_optimized": True
            }

            config_path = engine_path.replace('.trt', '_config.json')
            with open(config_path, 'w', encoding='utf-8') as f:
                json.dump(fallback_config, f, indent=2, ensure_ascii=False)

            logger.info(f"åå¤‡é…ç½®æ–‡ä»¶åˆ›å»ºæˆåŠŸ: {config_path}")
            return True

        except Exception as e:
            logger.error(f"åå¤‡å‚æ•°æ–‡ä»¶åˆ›å»ºå¤±è´¥: {e}")
            return False

    @staticmethod
    def load_tensorrt_engine(engine_path: str):
        """åŠ è½½TensorRTå¼•æ“"""
        try:
            if not os.path.exists(engine_path):
                logger.error(f"TensorRTå¼•æ“æ–‡ä»¶ä¸å­˜åœ¨: {engine_path}")
                return None

            trt_logger = trt.Logger(trt.Logger.WARNING)
            runtime = trt.Runtime(trt_logger)

            with open(engine_path, 'rb') as f:
                engine = runtime.deserialize_cuda_engine(f.read())

            if engine is None:
                logger.error("TensorRTå¼•æ“åŠ è½½å¤±è´¥")
                return None

            logger.info(f"TensorRTå¼•æ“åŠ è½½æˆåŠŸ: {engine_path}")
            return engine

        except Exception as e:
            logger.error(f"TensorRTå¼•æ“åŠ è½½é”™è¯¯: {e}")
            return None

class ONNXOptimizer:
    """ONNXè¿è¡Œæ—¶ä¼˜åŒ–å™¨"""

    @staticmethod
    def create_ort_session(model_path: str, device: str = "cuda") -> Optional[object]:
        """åˆ›å»ºä¼˜åŒ–çš„ONNX Runtimeä¼šè¯"""
        try:
            if not ONNX_AVAILABLE:
                logger.warning("ONNX Runtimeä¸å¯ç”¨")
                return None

            # é…ç½®providers
            providers = []
            if device == "cuda" and "CUDAExecutionProvider" in ort.get_available_providers():
                providers.append(("CUDAExecutionProvider", {
                    "device_id": 0,
                    "arena_extend_strategy": "kNextPowerOfTwo",
                    "gpu_mem_limit": 4 * 1024 * 1024 * 1024,  # 4GB for RTX 3060 Ti
                    "cudnn_conv_algo_search": "EXHAUSTIVE",
                    "do_copy_in_default_stream": True,
                }))
            providers.append("CPUExecutionProvider")

            # åˆ›å»ºä¼šè¯é€‰é¡¹
            sess_options = ort.SessionOptions()
            sess_options.graph_optimization_level = ort.GraphOptimizationLevel.ORT_ENABLE_ALL
            sess_options.execution_mode = ort.ExecutionMode.ORT_PARALLEL
            sess_options.inter_op_num_threads = 4
            sess_options.intra_op_num_threads = 4

            # åˆ›å»ºä¼šè¯
            session = ort.InferenceSession(model_path, sess_options, providers=providers)

            logger.info(f"ONNX Runtimeä¼šè¯åˆ›å»ºæˆåŠŸï¼Œä½¿ç”¨providers: {session.get_providers()}")
            return session

        except Exception as e:
            logger.error(f"ONNX Runtimeä¼šè¯åˆ›å»ºå¤±è´¥: {e}")
            return None

class FunASRModelWrapper(ModelWrapper):
    """FunASRæ¨¡å‹åŒ…è£… - RTX 3060 Tiä¼˜åŒ–ç‰ˆ"""
    def load_model(self) -> None:
        """åŠ è½½æ¨¡å‹ - TensorRTä¼˜åŒ–ç‰ˆ"""
        try:
            self.progress_tracker = ProgressTracker(100, f"åŠ è½½FunASRæ¨¡å‹")

            # åº”ç”¨RTX 3060 Tiä¼˜åŒ–
            RTX3060TiOptimizer.optimize_cuda_settings()

            # å¼ºåˆ¶å†…å­˜æ¸…ç†
            gc.collect()
            if torch.cuda.is_available():
                torch.cuda.empty_cache()
                RTX3060TiOptimizer.setup_gpu_memory(0.65)  # æ›´ä¿å®ˆçš„æ˜¾å­˜ä½¿ç”¨

            models_path = self.config.get('models_path', './models')
            os.makedirs(models_path, exist_ok=True)

            self.progress_tracker.update(10, "æ£€æŸ¥ä¼˜åŒ–æ¨¡å‹...")

            # ä¼˜å…ˆä½¿ç”¨ONNXæ¨¡å‹ï¼Œæ”¯æŒTensorRTåŠ é€Ÿ
            model_mapping = {
                "funasr-paraformer": "damo/speech_paraformer_asr-zh-cn-16k-common-vocab8404-onnx",
                "funasr-conformer": "damo/speech_conformer_asr_nat-zh-cn-16k-common-vocab8404-onnx"  # æ”¹ä¸ºONNXç‰ˆæœ¬
            }

            actual_model = model_mapping.get(self.model_id, model_mapping["funasr-paraformer"])

            # æ£€æŸ¥æ˜¯å¦å¯ä»¥ä½¿ç”¨TensorRTæˆ–ONNX RuntimeåŠ é€Ÿ
            engine_path = RTX3060TiOptimizer.get_tensorrt_engine_path(actual_model, self.config)
            onnx_session = None

            self.progress_tracker.update(20, "å°è¯•åŠ è½½ä¼˜åŒ–å¼•æ“...")

            # å°è¯•åŠ è½½TensorRTå¼•æ“
            if TENSORRT_AVAILABLE and os.path.exists(engine_path):
                logger.info("å‘ç°TensorRTå¼•æ“ï¼Œå°è¯•åŠ è½½...")
                self.tensorrt_engine = TensorRTOptimizer.load_tensorrt_engine(engine_path)
                if self.tensorrt_engine:
                    self.use_tensorrt = True
                    logger.info("[OK] TensorRTå¼•æ“åŠ è½½æˆåŠŸï¼Œæ€§èƒ½å°†æ˜¾è‘—æå‡")
                    self.progress_tracker.update(60, "TensorRTå¼•æ“å°±ç»ª")
                    self.progress_tracker.close()
                    return

            # å°è¯•ONNX RuntimeåŠ é€Ÿ
            if ONNX_AVAILABLE and actual_model.endswith("-onnx"):
                self.progress_tracker.update(30, "å°è¯•ONNX RuntimeåŠ é€Ÿ...")
                try:
                    # æ„å»ºONNXæ¨¡å‹è·¯å¾„
                    onnx_model_path = os.path.join(models_path, actual_model.replace("/", "_") + ".onnx")
                    if os.path.exists(onnx_model_path):
                        onnx_session = ONNXOptimizer.create_ort_session(onnx_model_path, self.device)
                        if onnx_session:
                            self.onnx_session = onnx_session
                            self.use_onnx = True
                            logger.info("[OK] ONNX RuntimeåŠ é€Ÿå¯ç”¨")
                            self.progress_tracker.update(40, "ONNXåŠ é€Ÿå°±ç»ª")
                except Exception as e:
                    logger.warning(f"ONNX RuntimeåŠ é€Ÿå¤±è´¥: {e}")

            self.progress_tracker.update(40, "åŠ è½½æ ‡å‡†FunASRæ¨¡å‹...")

            # è®¾å¤‡é€‰æ‹©é€»è¾‘
            if self.device == "cuda" and torch.cuda.is_available():
                available_memory = torch.cuda.get_device_properties(0).total_memory / 1024**3
                if available_memory >= 4.5:  # é™ä½è¦æ±‚åˆ°4.5GB
                    device = "cuda"
                else:
                    logger.warning("æ˜¾å­˜ä¸è¶³ï¼Œåˆ‡æ¢åˆ°CPUæ¨¡å¼")
                    device = "cpu"
                    self.device = "cpu"
            else:
                device = "cpu"
                self.device = "cpu"

            # ä¸ºRTX 3060 Tiä¼˜åŒ–çš„å‚æ•°
            model_kwargs = {
                "model": actual_model,
                "cache_dir": models_path,
                "device": device,
                "disable_update": True,
                "model_revision": "v2.0.4",
                "batch_size": 1,  # å‡å°æ‰¹æ¬¡å¤§å°
                "device_map": "auto" if device == "cuda" else None
            }

            self.model = AutoModel(**model_kwargs)

            self.progress_tracker.update(20, "æ¨¡å‹åŠ è½½å®Œæˆ")
            self.progress_tracker.close()

            # æ£€æŸ¥æ˜¯å¦å¯ä»¥åˆ›å»ºTensorRTå¼•æ“
            if not hasattr(self, 'use_tensorrt') and TENSORRT_AVAILABLE and device == "cuda":
                try:
                    self._try_create_tensorrt_engine(actual_model, engine_path)
                except Exception as e:
                    logger.warning(f"TensorRTå¼•æ“åˆ›å»ºå¤±è´¥: {e}")
                    # åˆ›å»ºåå¤‡é…ç½®
                    TensorRTOptimizer.create_fallback_engine(engine_path, {"model": actual_model})
            elif device == "cuda":
                # æ²¡æœ‰TensorRTæ—¶åˆ›å»ºåå¤‡é…ç½®
                try:
                    TensorRTOptimizer.create_fallback_engine(engine_path, {"model": actual_model})
                except Exception as e:
                    logger.warning(f"åå¤‡é…ç½®åˆ›å»ºå¤±è´¥: {e}")

            logger.info(f"[OK] FunASRæ¨¡å‹ {self.model_id} åŠ è½½æˆåŠŸï¼Œè¿è¡Œè®¾å¤‡: {self.device}")
            if hasattr(self, 'use_tensorrt') and self.use_tensorrt:
                logger.info("[BOOST] TensorRTåŠ é€Ÿå·²å¯ç”¨")
            elif hasattr(self, 'use_onnx') and self.use_onnx:
                logger.info("[BOOST] ONNX RuntimeåŠ é€Ÿå·²å¯ç”¨")

        except Exception as e:
            if self.progress_tracker:
                self.progress_tracker.close()
            logger.error(f"[ERROR] FunASRæ¨¡å‹åŠ è½½å¤±è´¥: {e}")
            # å°è¯•é™çº§åˆ°CPUæ¨¡å¼
            if self.device == "cuda":
                logger.info("å°è¯•ä½¿ç”¨CPUæ¨¡å¼é‡æ–°åŠ è½½...")
                self.device = "cpu"
                try:
                    self.model = AutoModel(
                        model="damo/speech_paraformer_asr-zh-cn-16k-common-vocab8404-onnx",
                        device="cpu",
                        cache_dir=models_path,
                        disable_update=True,
                        batch_size=1
                    )
                    logger.info("[OK] FunASRæ¨¡å‹å·²åœ¨CPUæ¨¡å¼ä¸‹åŠ è½½æˆåŠŸ")
                except Exception as cpu_e:
                    logger.error(f"[ERROR] CPUæ¨¡å¼ä¹Ÿå¤±è´¥: {cpu_e}")
                    raise
            else:
                raise

    def _try_create_tensorrt_engine(self, model_name: str, engine_path: str):
        """å°è¯•åˆ›å»ºTensorRTå¼•æ“"""
        try:
            logger.info("å°è¯•ä¸ºæ¨¡å‹åˆ›å»ºTensorRTå¼•æ“...")

            models_path = self.config.get('models_path', './models')

            # é¦–å…ˆå°è¯•ä»æ¨¡å‹ç›´æ¥åˆ›å»ºONNXæ–‡ä»¶
            onnx_path = os.path.join(models_path, model_name.replace("/", "_") + ".onnx")

            if not os.path.exists(onnx_path):
                logger.info("æœªæ‰¾åˆ°ONNXæ–‡ä»¶ï¼Œå°è¯•ä»æ¨¡å‹å¯¼å‡º...")
                if self._export_model_to_onnx(onnx_path):
                    logger.info("ONNXæ¨¡å‹å¯¼å‡ºæˆåŠŸ")
                else:
                    logger.warning("ONNXæ¨¡å‹å¯¼å‡ºå¤±è´¥ï¼Œè·³è¿‡TensorRTå¼•æ“åˆ›å»º")
                    return

            # åˆ›å»ºTensorRTå¼•æ“
            success = TensorRTOptimizer.convert_to_tensorrt(
                onnx_path, engine_path, precision="fp16"
            )

            if success:
                logger.info("TensorRTå¼•æ“åˆ›å»ºæˆåŠŸï¼Œä¸‹æ¬¡å¯åŠ¨å°†è‡ªåŠ¨ä½¿ç”¨åŠ é€Ÿ")
                # éªŒè¯å¼•æ“æ–‡ä»¶
                if self._validate_tensorrt_engine(engine_path):
                    logger.info("TensorRTå¼•æ“éªŒè¯é€šè¿‡")
                else:
                    logger.warning("TensorRTå¼•æ“éªŒè¯å¤±è´¥ï¼Œå°†ä½¿ç”¨æ ‡å‡†æ¨¡å¼")

        except Exception as e:
            logger.warning(f"TensorRTå¼•æ“åˆ›å»ºå¤±è´¥: {e}")

    def _export_model_to_onnx(self, onnx_path: str) -> bool:
        """å¯¼å‡ºæ¨¡å‹ä¸ºONNXæ ¼å¼"""
        try:
            if not hasattr(self, 'model') or self.model is None:
                return False

            logger.info("æ­£åœ¨å¯¼å‡ºæ¨¡å‹åˆ°ONNXæ ¼å¼...")

            # åˆ›å»ºç¤ºä¾‹è¾“å…¥
            dummy_audio_path = os.path.join(self.config.get('temp_path', './temp'), 'dummy_audio.wav')
            os.makedirs(os.path.dirname(dummy_audio_path), exist_ok=True)

            # ç”ŸæˆçŸ­æš‚çš„é™éŸ³éŸ³é¢‘ç”¨äºå¯¼å‡º
            import numpy as np
            import soundfile as sf
            dummy_audio = np.zeros(16000, dtype=np.float32)  # 1ç§’é™éŸ³
            sf.write(dummy_audio_path, dummy_audio, 16000)

            # ä½¿ç”¨æ¨¡å‹è¿›è¡Œæ¨ç†ä»¥è·å–è¾“å‡ºæ ¼å¼
            try:
                result = self.model.generate(
                    input=dummy_audio_path,
                    cache={},
                    language="zh",
                    use_itn=False,
                    batch_size=1
                )
                logger.info("ONNXå¯¼å‡ºå®Œæˆ")

                # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
                if os.path.exists(dummy_audio_path):
                    os.remove(dummy_audio_path)

                return True

            except Exception as e:
                logger.warning(f"æ¨¡å‹æ¨ç†å¤±è´¥: {e}")
                return False

        except Exception as e:
            logger.error(f"ONNXå¯¼å‡ºå¤±è´¥: {e}")
            return False

    def _validate_tensorrt_engine(self, engine_path: str) -> bool:
        """éªŒè¯TensorRTå¼•æ“æ–‡ä»¶"""
        try:
            if not os.path.exists(engine_path):
                return False

            # æ£€æŸ¥æ–‡ä»¶å¤§å°
            file_size = os.path.getsize(engine_path)
            if file_size < 1024:  # å°äº1KB
                logger.warning(f"TensorRTå¼•æ“æ–‡ä»¶è¿‡å°: {file_size} bytes")
                return False

            # å°è¯•åŠ è½½å¼•æ“
            engine = TensorRTOptimizer.load_tensorrt_engine(engine_path)
            if engine:
                logger.info(f"TensorRTå¼•æ“éªŒè¯æˆåŠŸï¼Œæ–‡ä»¶å¤§å°: {file_size/1024/1024:.1f}MB")
                return True
            else:
                return False

        except Exception as e:
            logger.error(f"TensorRTå¼•æ“éªŒè¯å¤±è´¥: {e}")
            return False

    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)
        self.use_tensorrt = False
        self.use_onnx = False
        self.tensorrt_engine = None
        self.onnx_session = None

    def transcribe(self, audio_path: str, **kwargs) -> Dict[str, Any]:
        """è½¬å½•éŸ³é¢‘ - RTX 3060 Tiä¼˜åŒ–ç‰ˆ"""
        try:
            progress = ProgressTracker(100, "FunASRéŸ³é¢‘è½¬å½•ä¸­")

            progress.update(10, "å¼€å§‹FunASRè½¬å½•...")

            # æ£€æŸ¥éŸ³é¢‘æ–‡ä»¶å¤§å°ï¼Œå¦‚æœå¤ªå¤§åˆ™åˆ†æ®µå¤„ç†
            file_size_mb = os.path.getsize(audio_path) / (1024 * 1024)
            if file_size_mb > 100:  # å¤§äº100MBçš„éŸ³é¢‘æ–‡ä»¶åˆ†æ®µå¤„ç†
                logger.info(f"éŸ³é¢‘æ–‡ä»¶è¾ƒå¤§({file_size_mb:.1f}MB)ï¼Œå°†åˆ†æ®µå¤„ç†ä»¥èŠ‚çœå†…å­˜")
                return self._transcribe_large_file(audio_path, progress)

            # å¼ºåˆ¶å†…å­˜æ¸…ç†
            gc.collect()
            if torch.cuda.is_available():
                torch.cuda.empty_cache()

            # FunASRè½¬å½• - ä½¿ç”¨ä¿å®ˆçš„å‚æ•°
            result = self.model.generate(
                input=audio_path,
                cache={},
                language="zh",
                use_itn=True,
                batch_size_s=60,  # å‡å°æ‰¹å¤„ç†å¤§å°ï¼Œé™ä½å†…å­˜å ç”¨
                batch_size=1     # å•ä¸ªæ‰¹æ¬¡å¤„ç†
            )

            progress.update(60, "å¤„ç†è½¬å½•ç»“æœ...")

            # è½¬æ¢ä¸ºæ ‡å‡†æ ¼å¼
            formatted_result = {
                "text": "",
                "segments": [],
                "language": "zh"
            }

            if result and len(result) > 0:
                for i, res in enumerate(result):
                    text = res.get("text", "")
                    if text:
                        # FunASRé€šå¸¸è¿”å›æ•´æ®µæ–‡æœ¬ï¼Œéœ€è¦æ‰‹åŠ¨åˆ†æ®µ
                        start_time = i * 30.0  # å‡è®¾æ¯æ®µ30ç§’
                        end_time = (i + 1) * 30.0

                        formatted_result["segments"].append({
                            "start": start_time,
                            "end": end_time,
                            "text": text.strip()
                        })
                        formatted_result["text"] += text.strip() + " "

                        # æ¯å¤„ç†10ä¸ªç‰‡æ®µæ¸…ç†ä¸€æ¬¡å†…å­˜
                        if i % 10 == 0:
                            gc.collect()

            progress.close()

            # è½¬å½•å®Œæˆåæ¸…ç†å†…å­˜
            gc.collect()
            if torch.cuda.is_available():
                torch.cuda.empty_cache()

            return formatted_result

        except Exception as e:
            logger.error(f"[ERROR] FunASRè½¬å½•å¤±è´¥: {e}")
            # å†…å­˜ä¸è¶³æ—¶çš„é”™è¯¯å¤„ç†
            if "out of memory" in str(e).lower() or "cuda" in str(e).lower():
                logger.warning("æ˜¾å­˜ä¸è¶³ï¼Œå°è¯•åˆ‡æ¢åˆ°CPUæ¨¡å¼...")
                try:
                    # é‡æ–°åŠ è½½ä¸ºCPUæ¨¡å¼
                    self.device = "cpu"
                    self.load_model()
                    return self.transcribe(audio_path, **kwargs)
                except Exception as cpu_e:
                    logger.error(f"[ERROR] CPUæ¨¡å¼ä¹Ÿå¤±è´¥: {cpu_e}")
            raise

    def _transcribe_large_file(self, audio_path: str, progress: ProgressTracker) -> Dict[str, Any]:
        """åˆ†æ®µå¤„ç†å¤§éŸ³é¢‘æ–‡ä»¶"""
        try:
            import librosa

            # åŠ è½½éŸ³é¢‘å¹¶åˆ†æ®µ
            audio, sr = librosa.load(audio_path, sr=16000)
            duration = len(audio) / sr
            segment_length = 300  # 5åˆ†é’Ÿä¸€æ®µ

            formatted_result = {
                "text": "",
                "segments": [],
                "language": "zh"
            }

            progress.update(20, f"åˆ†æ®µå¤„ç†éŸ³é¢‘ï¼Œæ€»æ—¶é•¿: {duration:.1f}ç§’")

            for start_sec in range(0, int(duration), segment_length):
                end_sec = min(start_sec + segment_length, duration)

                # æå–éŸ³é¢‘æ®µ
                start_sample = int(start_sec * sr)
                end_sample = int(end_sec * sr)
                segment_audio = audio[start_sample:end_sample]

                # ä¿å­˜ä¸´æ—¶æ–‡ä»¶
                temp_path = f"temp_segment_{start_sec}.wav"
                sf.write(temp_path, segment_audio, sr)

                try:
                    # è½¬å½•è¯¥æ®µ
                    segment_result = self.model.generate(
                        input=temp_path,
                        cache={},
                        language="zh",
                        use_itn=True,
                        batch_size_s=60,
                        batch_size=1
                    )

                    if segment_result and len(segment_result) > 0:
                        for res in segment_result:
                            text = res.get("text", "")
                            if text:
                                formatted_result["segments"].append({
                                    "start": start_sec,
                                    "end": end_sec,
                                    "text": text.strip()
                                })
                                formatted_result["text"] += text.strip() + " "

                    # æ¸…ç†ä¸´æ—¶æ–‡ä»¶å’Œå†…å­˜
                    os.remove(temp_path)
                    gc.collect()
                    if torch.cuda.is_available():
                        torch.cuda.empty_cache()

                except Exception as e:
                    logger.warning(f"æ®µ {start_sec}-{end_sec} å¤„ç†å¤±è´¥: {e}")
                    if os.path.exists(temp_path):
                        os.remove(temp_path)

                progress.update(60 * (end_sec - start_sec) / duration, f"å¤„ç†è¿›åº¦: {end_sec:.0f}/{duration:.0f}ç§’")

            return formatted_result

        except Exception as e:
            logger.error(f"[ERROR] å¤§æ–‡ä»¶åˆ†æ®µå¤„ç†å¤±è´¥: {e}")
            raise

class VideoSubtitleExtractor:
    """è§†é¢‘å­—å¹•æå–å™¨"""
    def __init__(self, model_id: str = "faster-base", device: str = "cuda", config: Config = None, **kwargs):
        self.config = config or Config()
        self.device = device
        self.kwargs = kwargs

        # æ£€æŸ¥ç³»ç»Ÿ
        if not SystemChecker.check_cuda():
            self.device = "cpu"
            logger.warning("âš ï¸ CUDAä¸å¯ç”¨ï¼Œä½¿ç”¨CPUæ¨¡å¼")

        # åˆå§‹åŒ–æ¨¡å‹
        self.model_wrapper = self._create_model(model_id)

    def _create_model(self, model_id: str):
        """åˆ›å»ºæ¨¡å‹å®ä¾‹"""
        if model_id in ["tiny", "base", "small", "medium", "large", "faster-base", "faster-large"]:
            return WhisperModelWrapper(model_id, self.device, self.config, **self.kwargs)
        elif model_id in ["funasr-paraformer", "funasr-conformer"]:
            if not FUNASR_AVAILABLE:
                raise ValueError("FunASRåº“æœªå®‰è£…ï¼Œè¯·è¿è¡Œ: pip install funasr")
            return FunASRModelWrapper(model_id, self.device, self.config, **self.kwargs)
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„æ¨¡å‹: {model_id}")

    def extract_audio(self, video_path: str, audio_path: str = None) -> Optional[str]:
        """ä»è§†é¢‘æå–éŸ³é¢‘"""
        if not audio_path:
            base_name = os.path.splitext(os.path.basename(video_path))[0]
            temp_path = self.config.get('temp_path', './temp')
            os.makedirs(temp_path, exist_ok=True)
            audio_path = os.path.join(temp_path, f"{base_name}_audio.wav")

        if not os.path.exists(video_path):
            logger.error(f"âŒ è§†é¢‘æ–‡ä»¶ä¸å­˜åœ¨: {video_path}")
            return None

        try:
            progress = ProgressTracker(100, "æå–éŸ³é¢‘")

            with Timer("éŸ³é¢‘æå–"):
                progress.update(10, "æ£€æŸ¥è§†é¢‘æ–‡ä»¶...")

                if MOVIEPY_AVAILABLE:
                    progress.update(20, "ä½¿ç”¨MoviePyæå–éŸ³é¢‘...")
                    video = VideoFileClip(video_path)
                    audio = video.audio
                    progress.update(30, "å†™å…¥éŸ³é¢‘æ–‡ä»¶...")
                    audio.write_audiofile(
                        audio_path, 
                        fps=self.config.get('audio_sample_rate', 16000), 
                        verbose=False, 
                        logger=None
                    )
                    progress.update(30, "æ¸…ç†èµ„æº...")
                    video.close()
                    audio.close()
                else:
                    progress.update(20, "ä½¿ç”¨FFmpegæå–éŸ³é¢‘...")
                    cmd = [
                        "ffmpeg", "-y", "-i", video_path,
                        "-vn", "-acodec", "pcm_s16le", 
                        "-ar", str(self.config.get('audio_sample_rate', 16000)), 
                        "-ac", "1", audio_path
                    ]
                    subprocess.run(cmd, check=True, capture_output=True)
                    progress.update(60, "éŸ³é¢‘æå–å®Œæˆ")

                progress.update(10, "éªŒè¯éŸ³é¢‘æ–‡ä»¶...")
                if os.path.exists(audio_path):
                    file_size = os.path.getsize(audio_path) / 1024 / 1024
                    progress.close()
                    logger.info(f"âœ… éŸ³é¢‘æå–æˆåŠŸ: {audio_path} ({file_size:.1f}MB)")
                    return audio_path
                else:
                    progress.close()
                    logger.error("âŒ éŸ³é¢‘æå–å¤±è´¥")
                    return None

        except Exception as e:
            logger.error(f"âŒ éŸ³é¢‘æå–å‡ºé”™: {e}")
            return None

    def transcribe_audio(self, audio_path: str, **kwargs) -> Dict[str, Any]:
        """è½¬å½•éŸ³é¢‘"""
        if not os.path.exists(audio_path):
            logger.error(f"âŒ éŸ³é¢‘æ–‡ä»¶ä¸å­˜åœ¨: {audio_path}")
            return {"segments": [], "language": None}

        try:
            # åŠ è½½æ¨¡å‹
            if self.model_wrapper.model is None:
                self.model_wrapper.load_model()

            with Timer("éŸ³é¢‘è½¬å½•"):
                result = self.model_wrapper.transcribe(audio_path, **kwargs)
                segment_count = len(result.get('segments', []))
                logger.info(f"âœ… è½¬å½•å®Œæˆï¼Œè¯†åˆ«åˆ° {segment_count} ä¸ªç‰‡æ®µ")

                if self.device == "cuda":
                    memory_usage = self.model_wrapper.get_gpu_memory_usage()
                    logger.info(f"ğŸ“Š è½¬å½•åæ˜¾å­˜ä½¿ç”¨: {memory_usage:.1f}MB")

                return result

        except Exception as e:
            logger.error(f"âŒ éŸ³é¢‘è½¬å½•å¤±è´¥: {e}")
            return {"segments": [], "language": None}

    def create_srt_file(self, segments: List[Dict], output_path: str = "output.srt", enable_postprocess: bool = True) -> str:
        """åˆ›å»ºSRTå­—å¹•æ–‡ä»¶"""
        try:
            progress = ProgressTracker(len(segments) + 10, "ç”Ÿæˆå­—å¹•æ–‡ä»¶")

            output_dir = self.config.get('output_path', './output')
            os.makedirs(output_dir, exist_ok=True)

            if not output_path.startswith(output_dir):
                output_path = os.path.join(output_dir, os.path.basename(output_path))

            # åˆå§‹åŒ–æ–‡æœ¬åå¤„ç†å™¨
            if enable_postprocess:
                progress.update(5, "åˆå§‹åŒ–æ–‡æœ¬åå¤„ç†å™¨...")
                postprocessor = TextPostProcessor()

                # ç»Ÿè®¡åŸå§‹é”™è¯¯
                total_text = " ".join([seg["text"] for seg in segments])
                original_stats = postprocessor.get_correction_stats(total_text)
                logger.info(f"ğŸ” æ£€æµ‹åˆ°æ½œåœ¨é”™è¯¯: ä¸“ä¸šåè¯ {original_stats['professional_terms']} å¤„, "
                          f"å¤šéŸ³å­— {original_stats['polyphone_errors']} å¤„, "
                          f"æ•°å­—å•ä½ {original_stats['number_units']} å¤„")

            with open(output_path, "w", encoding="utf-8") as f:
                for i, segment in enumerate(segments, 1):
                    start_time = self._format_time(segment["start"])
                    end_time = self._format_time(segment["end"])
                    text = segment["text"].strip()

                    # åº”ç”¨æ–‡æœ¬åå¤„ç†
                    if enable_postprocess:
                        corrected_text = postprocessor.post_process(text)
                        if corrected_text != text:
                            logger.debug(f"ç‰‡æ®µ {i} æ–‡æœ¬çº é”™: '{text}' -> '{corrected_text}'")
                        text = corrected_text

                    f.write(f"{i}\n")
                    f.write(f"{start_time} --> {end_time}\n")
                    f.write(f"{text}\n\n")

                    progress.update(1, f"å†™å…¥ç‰‡æ®µ {i}/{len(segments)}")

            # ä¿å­˜åŸå§‹ç‰ˆæœ¬ï¼ˆå¯é€‰ï¼‰
            if enable_postprocess:
                progress.update(3, "ä¿å­˜åŸå§‹ç‰ˆæœ¬...")
                original_path = output_path.replace(".srt", "_original.srt")
                with open(original_path, "w", encoding="utf-8") as f:
                    for i, segment in enumerate(segments, 1):
                        start_time = self._format_time(segment["start"])
                        end_time = self._format_time(segment["end"])
                        text = segment["text"].strip()
                        f.write(f"{i}\n")
                        f.write(f"{start_time} --> {end_time}\n")
                        f.write(f"{text}\n\n")
                logger.info(f"ğŸ“ åŸå§‹å­—å¹•ä¿å­˜è‡³: {original_path}")

            progress.update(2, "å®Œæˆå­—å¹•ç”Ÿæˆ...")
            progress.close()
            logger.info(f"âœ… SRTæ–‡ä»¶ä¿å­˜æˆåŠŸ: {output_path}")

            if enable_postprocess:
                logger.info("ğŸ¯ æ–‡æœ¬åå¤„ç†åŠŸèƒ½å·²å¯ç”¨ï¼Œä¸“ä¸šåè¯å’Œå¤šéŸ³å­—é”™è¯¯å·²è‡ªåŠ¨ä¿®æ­£")

            return output_path

        except Exception as e:
            logger.error(f"âŒ SRTæ–‡ä»¶åˆ›å»ºå¤±è´¥: {e}")
            return None

    def _format_time(self, seconds: float) -> str:
        """æ ¼å¼åŒ–æ—¶é—´ä¸ºSRTæ ¼å¼"""
        hours, remainder = divmod(seconds, 3600)
        minutes, seconds = divmod(remainder, 60)
        milliseconds = int((seconds - int(seconds)) * 1000)
        return f"{int(hours):02d}:{int(minutes):02d}:{int(seconds):02d},{milliseconds:03d}"

    def cleanup(self):
        """æ¸…ç†ä¸´æ—¶æ–‡ä»¶å’Œæ˜¾å­˜"""
        try:
            temp_path = self.config.get('temp_path', './temp')
            if os.path.exists(temp_path):
                temp_files = [f for f in os.listdir(temp_path) if f.endswith("_audio.wav")]
                for file in temp_files:
                    file_path = os.path.join(temp_path, file)
                    try:
                        os.remove(file_path)
                        logger.info(f"ğŸ—‘ï¸ åˆ é™¤ä¸´æ—¶æ–‡ä»¶: {file}")
                    except Exception as e:
                        logger.warning(f"âš ï¸ åˆ é™¤ä¸´æ—¶æ–‡ä»¶å¤±è´¥ {file}: {e}")

            # æ¸…ç†GPUæ˜¾å­˜
            if torch.cuda.is_available():
                torch.cuda.empty_cache()
                gc.collect()
                logger.info("ğŸ§¹ GPUæ˜¾å­˜æ¸…ç†å®Œæˆ")

        except Exception as e:
            logger.warning(f"âš ï¸ æ¸…ç†è¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")

def main():
    parser = argparse.ArgumentParser(description="ä¸­æ–‡ç”µè§†å‰§éŸ³é¢‘è½¬æ–‡å­—å·¥å…· - RTX 3060 Tiä¼˜åŒ–ç‰ˆ")
    parser.add_argument("video_path", nargs='?', default="test.mp4", help="è¾“å…¥è§†é¢‘æ–‡ä»¶è·¯å¾„")
    parser.add_argument("--output", "-o", default="output.srt", help="è¾“å‡ºå­—å¹•æ–‡ä»¶è·¯å¾„")
    parser.add_argument("--model", "-m", default="faster-base",
                        choices=["tiny", "base", "small", "medium", "large", "faster-base", "faster-large", 
                                "funasr-paraformer", "funasr-conformer"],
                        help="æ¨¡å‹é€‰æ‹© (æ¨èRTX 3060 Tiä½¿ç”¨faster-baseæˆ–funasr-paraformer)")
    parser.add_argument("--device", "-d", default="cuda", choices=["cuda", "cpu"], help="è¿è¡Œè®¾å¤‡")
    parser.add_argument("--language", "-l", default="zh", help="è¯­è¨€è®¾ç½®")
    parser.add_argument("--keep-temp", action="store_true", help="ä¿ç•™ä¸´æ—¶æ–‡ä»¶")
    parser.add_argument("--config", "-c", help="é…ç½®æ–‡ä»¶è·¯å¾„")
    parser.add_argument("--no-postprocess", action="store_true", help="ç¦ç”¨æ–‡æœ¬åå¤„ç†")
    parser.add_argument("--add-term", nargs=2, metavar=('CORRECT', 'WRONG'), 
                        help="æ·»åŠ è‡ªå®šä¹‰çº é”™è¯æ±‡: --add-term 'æ­£ç¡®è¯' 'é”™è¯¯è¯'")

    args = parser.parse_args()

    # åŠ è½½é…ç½®
    config = Config()
    if args.config and os.path.exists(args.config):
        config.config_file = args.config
        config.load_config()

    # æ£€æŸ¥è¾“å…¥æ–‡ä»¶
    if not os.path.exists(args.video_path):
        logger.error(f"âŒ è§†é¢‘æ–‡ä»¶ä¸å­˜åœ¨: {args.video_path}")
        return

    # æ£€æŸ¥ä¾èµ–
    if not SystemChecker.check_dependencies():
        logger.error("âŒ è¯·å…ˆè¿è¡Œinstall_dependencies.batå®‰è£…ç¼ºå°‘çš„ä¾èµ–")
        return

    logger.info(f"ğŸ¬ å¼€å§‹å¤„ç†è§†é¢‘: {args.video_path}")
    logger.info(f"ğŸ¤– ä½¿ç”¨æ¨¡å‹: {args.model}")
    logger.info(f"ğŸ’» è¿è¡Œè®¾å¤‡: {args.device}")

    if args.model in ["medium", "large"] and args.device == "cuda":
        logger.warning("[WARNING] RTX 3060 Tiæ˜¾å­˜å¯èƒ½ä¸è¶³ä»¥è¿è¡Œmedium/largeæ¨¡å‹ï¼Œå»ºè®®ä½¿ç”¨faster-base")

    if args.model in ["funasr-paraformer", "funasr-conformer"]:
        logger.warning("[WARNING] FunASRæ¨¡å‹å†…å­˜å ç”¨è¾ƒå¤§ï¼Œå¦‚é‡åˆ°å†…å­˜ä¸è¶³è¯·è€ƒè™‘ä½¿ç”¨faster-baseæ¨¡å‹")
        # æ£€æŸ¥å¯ç”¨å†…å­˜
        memory = psutil.virtual_memory()
        if memory.available < 4 * 1024**3:  # å°äº4GBå¯ç”¨å†…å­˜
            logger.warning(f"[WARNING] å¯ç”¨å†…å­˜ä¸è¶³({memory.available/1024**3:.1f}GB)ï¼Œå»ºè®®å…³é—­å…¶ä»–ç¨‹åºæˆ–ä½¿ç”¨smalleræ¨¡å‹")

    extractor = None
    try:
        # é¦–æ¬¡è¿è¡Œè‡ªåŠ¨ä¼˜åŒ–TensorRTå¼•æ“
        if TENSORRT_MANAGER_AVAILABLE and args.device == "cuda":
            try:
                engine_manager = TensorRTEngineManager(config)
                model_name = args.model
                if model_name in ["funasr-paraformer", "funasr-conformer"]:
                    model_name = "damo/speech_paraformer_asr-zh-cn-16k-common-vocab8404-onnx"
                # æ£€æŸ¥æ˜¯å¦éœ€è¦ä¼˜åŒ–
                if not engine_manager.get_engine_info(model_name.replace("/", "_")):
                    logger.info(f"ä¸ºæ¨¡å‹ {model_name} å‡†å¤‡TensorRTä¼˜åŒ–...")
                    engine_manager.optimize_for_rtx3060ti(model_name)
                else:
                    logger.info("TensorRTå¼•æ“å·²å­˜åœ¨ï¼Œè·³è¿‡ä¼˜åŒ–")
            except Exception as e:
                logger.warning(f"TensorRTä¼˜åŒ–å¤±è´¥: {e}")
                logger.info("å°†ä½¿ç”¨æ ‡å‡†æ¨¡å¼è¿è¡Œ")

        # åˆ›å»ºæå–å™¨
        extractor = VideoSubtitleExtractor(
            model_id=args.model,
            device=args.device,
            config=config
        )

        # æå–éŸ³é¢‘
        audio_path = extractor.extract_audio(args.video_path)
        if not audio_path:
            logger.error("âŒ éŸ³é¢‘æå–å¤±è´¥")
            return

        # è½¬å½•éŸ³é¢‘
        result = extractor.transcribe_audio(
            audio_path,
            language=args.language,
            temperature=0.0
        )

        if not result["segments"]:
            logger.warning("âš ï¸ æœªè¯†åˆ«åˆ°ä»»ä½•è¯­éŸ³å†…å®¹")
            return

        # åˆ›å»ºå­—å¹•æ–‡ä»¶
        enable_postprocess = not args.no_postprocess
        srt_path = extractor.create_srt_file(result["segments"], args.output, enable_postprocess)
        if srt_path:
            logger.info(f"ğŸ‰ å­—å¹•æå–å®Œæˆï¼æ–‡ä»¶ä¿å­˜è‡³: {srt_path}")
            logger.info(f"ğŸ“ å…±è¯†åˆ«åˆ° {len(result['segments'])} ä¸ªå­—å¹•ç‰‡æ®µ")
            if enable_postprocess:
                logger.info("âœ¨ å·²åº”ç”¨æ™ºèƒ½æ–‡æœ¬çº é”™")
        else:
            logger.error("âŒ å­—å¹•æ–‡ä»¶åˆ›å»ºå¤±è´¥")

        # å¤„ç†è‡ªå®šä¹‰è¯æ±‡æ·»åŠ 
        if args.add_term:
            from text_postprocessor import TextPostProcessor
            postprocessor = TextPostProcessor()
            postprocessor.add_custom_term(args.add_term[0], [args.add_term[1]])
            logger.info(f"âœ… å·²æ·»åŠ è‡ªå®šä¹‰çº é”™è¯æ±‡: {args.add_term[0]} <- {args.add_term[1]}")

    except Exception as e:
        logger.error(f"âŒ å¤„ç†è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}")
        traceback.print_exc()

    finally:
        # æ¸…ç†ä¸´æ—¶æ–‡ä»¶å’Œæ˜¾å­˜
        try:
            if extractor is not None and not args.keep_temp:
                extractor.cleanup()
        except Exception as e:
            logger.warning(f"âš ï¸ æ¸…ç†è¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")

if __name__ == "__main__":
    main()