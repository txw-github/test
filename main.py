
#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
RTX 3060 Ti ä¸­æ–‡è§†é¢‘è½¬å­—å¹•å·¥å…·
æ”¯æŒå¤šç§æ¨¡å‹ï¼šWhisperã€Faster-Whisperã€FunASR
é’ˆå¯¹ä¸­æ–‡ç”µè§†å‰§ä¼˜åŒ–ï¼Œæ”¯æŒTensorRTåŠ é€Ÿ
"""

import os
import sys
import logging
import argparse
import time
import json
from pathlib import Path
from typing import Optional, Dict, List, Any
from tqdm import tqdm
import subprocess

# è®¾ç½®ç¯å¢ƒå˜é‡
os.environ['CUDA_LAZY_LOADING'] = '1'
os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.StreamHandler(sys.stdout),
        logging.FileHandler('conversion.log', encoding='utf-8')
    ]
)
logger = logging.getLogger(__name__)

# è®¾ç½®FFmpegè·¯å¾„ (Replitç¯å¢ƒé€‚é…)
ffmpeg_paths = [
    r"D:\code\ffmpeg\bin",  # Windowsæœ¬åœ°è·¯å¾„
    "/usr/bin",             # Linuxç³»ç»Ÿè·¯å¾„
    "/usr/local/bin"        # å¤‡ç”¨è·¯å¾„
]

for ffmpeg_path in ffmpeg_paths:
    if os.path.exists(ffmpeg_path):
        if ffmpeg_path not in os.environ.get("PATH", ""):
            os.environ["PATH"] += os.pathsep + ffmpeg_path
        break

# æ¨¡å‹é…ç½®
SUPPORTED_MODELS = {
    # Faster-Whisper æ¨¡å‹ (æ¨èï¼Œé€Ÿåº¦å¿«5å€)
    "faster-tiny": {
        "size": "39MB", 
        "model_id": "guillaumekln/faster-whisper-tiny",
        "description": "æœ€å°æ¨¡å‹ï¼Œé€Ÿåº¦æœ€å¿«ï¼Œè´¨é‡ä¸€èˆ¬",
        "vram": "0.5GB",
        "rtx3060ti": "excellent"
    },
    "faster-base": {
        "size": "142MB", 
        "model_id": "guillaumekln/faster-whisper-base",
        "description": "åŸºç¡€æ¨¡å‹ï¼Œé€Ÿåº¦ä¸è´¨é‡å¹³è¡¡",
        "vram": "1GB",
        "rtx3060ti": "excellent"
    },
    "faster-small": {
        "size": "461MB", 
        "model_id": "guillaumekln/faster-whisper-small",
        "description": "å°æ¨¡å‹ï¼Œè¾ƒå¥½çš„è´¨é‡",
        "vram": "1.5GB",
        "rtx3060ti": "excellent"
    },
    "faster-medium": {
        "size": "1.5GB", 
        "model_id": "guillaumekln/faster-whisper-medium",
        "description": "ä¸­ç­‰æ¨¡å‹ï¼Œè‰¯å¥½è´¨é‡",
        "vram": "3GB",
        "rtx3060ti": "good"
    },
    "faster-large": {
        "size": "2.9GB", 
        "model_id": "guillaumekln/faster-whisper-large-v2",
        "description": "å¤§æ¨¡å‹v2ï¼Œé«˜è´¨é‡",
        "vram": "4GB",
        "rtx3060ti": "limited"
    },
    "faster-large-v2": {
        "size": "2.9GB", 
        "model_id": "guillaumekln/faster-whisper-large-v2",
        "description": "å¤§æ¨¡å‹v2ï¼Œä¸“ä¸šè´¨é‡ï¼Œä¸­æ–‡ä¼˜åŒ–",
        "vram": "4GB",
        "rtx3060ti": "limited"
    },
    "faster-large-v3": {
        "size": "2.9GB", 
        "model_id": "guillaumekln/faster-whisper-large-v3",
        "description": "æœ€æ–°å¤§æ¨¡å‹v3ï¼Œæœ€é«˜è´¨é‡ï¼Œå¤šè¯­è¨€ä¼˜åŒ–",
        "vram": "4.5GB",
        "rtx3060ti": "limited"
    },
    
    # æ ‡å‡† Whisper æ¨¡å‹
    "tiny": {
        "size": "39MB", 
        "model_id": "tiny",
        "description": "OpenAIåŸç‰ˆæœ€å°æ¨¡å‹",
        "vram": "0.5GB",
        "rtx3060ti": "excellent"
    },
    "base": {
        "size": "142MB", 
        "model_id": "base",
        "description": "OpenAIåŸç‰ˆåŸºç¡€æ¨¡å‹",
        "vram": "1GB",
        "rtx3060ti": "excellent"
    },
    "small": {
        "size": "461MB", 
        "model_id": "small",
        "description": "OpenAIåŸç‰ˆå°æ¨¡å‹",
        "vram": "1.5GB",
        "rtx3060ti": "excellent"
    },
    "medium": {
        "size": "1.5GB", 
        "model_id": "medium",
        "description": "OpenAIåŸç‰ˆä¸­ç­‰æ¨¡å‹",
        "vram": "3GB",
        "rtx3060ti": "good"
    },
    "large": {
        "size": "2.9GB", 
        "model_id": "large",
        "description": "OpenAIåŸç‰ˆå¤§æ¨¡å‹(v1)",
        "vram": "4GB",
        "rtx3060ti": "limited"
    },
    "large-v2": {
        "size": "2.9GB", 
        "model_id": "large-v2",
        "description": "OpenAIåŸç‰ˆå¤§æ¨¡å‹v2ï¼Œæ”¹è¿›çš„ä¸­æ–‡å’Œå¤šè¯­è¨€æ”¯æŒ",
        "vram": "4GB",
        "rtx3060ti": "limited",
        "features": ["improved_chinese", "better_punctuation", "reduced_hallucination"]
    },
    "large-v3": {
        "size": "2.9GB", 
        "model_id": "large-v3",
        "description": "OpenAIåŸç‰ˆå¤§æ¨¡å‹v3ï¼Œæœ€æ–°ç‰ˆæœ¬ï¼Œæœ€ä½³è´¨é‡",
        "vram": "4.5GB",
        "rtx3060ti": "limited",
        "features": ["best_quality", "multilingual", "robust_audio", "timestamp_accuracy"]
    },
    
    # ä¸­æ–‡ä¼˜åŒ–æ¨¡å‹
    "chinese-whisper-small": {
        "size": "461MB", 
        "model_id": "small",
        "description": "ä¸­æ–‡ä¼˜åŒ–çš„å°æ¨¡å‹",
        "vram": "1.5GB",
        "rtx3060ti": "excellent"
    },
    "chinese-whisper-base": {
        "size": "142MB", 
        "model_id": "base",
        "description": "ä¸­æ–‡ä¼˜åŒ–çš„åŸºç¡€æ¨¡å‹",
        "vram": "1GB",
        "rtx3060ti": "excellent"
    },
}

# æ£€æŸ¥ä¾èµ–
try:
    import torch
    import torchaudio
    logger.info(f"PyTorchç‰ˆæœ¬: {torch.__version__}")
    if torch.cuda.is_available():
        logger.info(f"CUDAç‰ˆæœ¬: {torch.version.cuda}")
        logger.info(f"GPU: {torch.cuda.get_device_name(0)}")
    else:
        logger.warning("CUDAä¸å¯ç”¨ï¼Œå°†ä½¿ç”¨CPUæ¨¡å¼")
except ImportError:
    logger.error("PyTorchæœªå®‰è£…ï¼è¯·è¿è¡Œ: pip install torch torchaudio --index-url https://download.pytorch.org/whl/cu121")
    sys.exit(1)

try:
    from faster_whisper import WhisperModel as FasterWhisperModel
    FASTER_WHISPER_AVAILABLE = True
except ImportError:
    FASTER_WHISPER_AVAILABLE = False
    logger.warning("Faster-Whisperæœªå®‰è£…")

try:
    import whisper
    WHISPER_AVAILABLE = True
except ImportError:
    WHISPER_AVAILABLE = False
    logger.warning("OpenAI Whisperæœªå®‰è£…")

try:
    import moviepy.editor as mp
    MOVIEPY_AVAILABLE = True
except ImportError:
    MOVIEPY_AVAILABLE = False
    logger.warning("MoviePyæœªå®‰è£…")

try:
    import jieba
    JIEBA_AVAILABLE = True
except ImportError:
    JIEBA_AVAILABLE = False
    logger.warning("Jiebaæœªå®‰è£…ï¼Œä¸­æ–‡åˆ†è¯åŠŸèƒ½ä¸å¯ç”¨")

try:
    from huggingface_hub import hf_hub_download
    from huggingface_hub.utils import HfHubHTTPError
    HF_HUB_AVAILABLE = True
except ImportError:
    HF_HUB_AVAILABLE = False
    logger.warning("Hugging Face Hubæœªå®‰è£…")

# TensorRTæ”¯æŒæ£€æŸ¥ (Replitç¯å¢ƒé€šå¸¸ä¸æ”¯æŒ)
try:
    import tensorrt as trt
    import pycuda.driver as cuda
    import pycuda.autoinit
    TENSORRT_AVAILABLE = True
    logger.info(f"TensorRTç‰ˆæœ¬: {trt.__version__}")
except ImportError:
    TENSORRT_AVAILABLE = False
    # åœ¨Replitç¯å¢ƒä¸­ï¼ŒTensorRTé€šå¸¸ä¸å¯ç”¨ï¼Œè¿™æ˜¯æ­£å¸¸çš„
    pass

try:
    import onnx
    import onnxruntime
    ONNX_AVAILABLE = True
    logger.info("ONNXè¿è¡Œæ—¶å¯ç”¨")
except ImportError:
    ONNX_AVAILABLE = False
    logger.warning("ONNXæœªå®‰è£…")


class Config:
    """é…ç½®ç±»"""
    def __init__(self, **kwargs):
        # åŸºç¡€é…ç½®
        self.rtx_3060_ti_optimized = kwargs.get('rtx_3060_ti_optimized', True)
        self.max_memory_usage = kwargs.get('max_memory_usage', 0.8)
        self.batch_size = kwargs.get('batch_size', 16)
        self.chunk_length = kwargs.get('chunk_length', 30)
        self.device = kwargs.get('device', "cuda" if torch.cuda.is_available() else "cpu")
        
        # éŸ³é¢‘å¤„ç†é…ç½®
        self.audio_quality = kwargs.get('audio_quality', 'balanced')
        self.enable_audio_preprocessing = kwargs.get('enable_audio_preprocessing', True)
        self.sample_rate = kwargs.get('sample_rate', 16000)
        
        # æ¨¡å‹é…ç½®
        self.model_cache_dir = kwargs.get('model_cache_dir', './models')
        self.compute_type = kwargs.get('compute_type', 'auto')
        self.beam_size = kwargs.get('beam_size', 5)
        self.best_of = kwargs.get('best_of', 5)
        self.temperature = kwargs.get('temperature', 0.0)
        
        # æ–‡æœ¬å¤„ç†é…ç½®
        self.enable_text_correction = kwargs.get('enable_text_correction', True)
        self.language = kwargs.get('language', 'zh')
        self.verbose = kwargs.get('verbose', False)
        
        # TensorRTåŠ é€Ÿé…ç½®
        self.enable_tensorrt = kwargs.get('enable_tensorrt', True)
        self.tensorrt_precision = kwargs.get('tensorrt_precision', 'fp16')  # fp16, fp32, int8
        self.tensorrt_workspace_size = kwargs.get('tensorrt_workspace_size', 1024)  # MB
        self.tensorrt_max_batch_size = kwargs.get('tensorrt_max_batch_size', 8)
        
        # è¾“å‡ºé…ç½®
        self.output_format = kwargs.get('output_format', 'srt')
        self.keep_temp = kwargs.get('keep_temp', False)
        
        # RTX 3060 Ti ç‰¹å®šä¼˜åŒ–
        if torch.cuda.is_available():
            gpu_name = torch.cuda.get_device_name(0)
            if "3060 Ti" in gpu_name:
                self.batch_size = kwargs.get('batch_size', 8)
                self.chunk_length = kwargs.get('chunk_length', 20)
                logger.info("æ£€æµ‹åˆ°RTX 3060 Tiï¼Œå·²åº”ç”¨ä¼˜åŒ–è®¾ç½®")
        
        # åˆ›å»ºæ¨¡å‹ç›®å½•
        os.makedirs(self.model_cache_dir, exist_ok=True)


class TensorRTOptimizer:
    """TensorRTä¼˜åŒ–å™¨"""
    
    def __init__(self, config: Config):
        self.config = config
        self.trt_cache_dir = Path(config.model_cache_dir) / "tensorrt"
        self.trt_cache_dir.mkdir(exist_ok=True)
    
    def optimize_model(self, model_path: str, model_name: str) -> str:
        """å°†æ¨¡å‹è½¬æ¢ä¸ºTensorRTä¼˜åŒ–ç‰ˆæœ¬"""
        if not TENSORRT_AVAILABLE:
            logger.warning("TensorRTä¸å¯ç”¨ï¼Œè·³è¿‡ä¼˜åŒ–")
            return model_path
        
        trt_model_path = self.trt_cache_dir / f"{model_name}_trt.engine"
        
        if trt_model_path.exists():
            logger.info(f"TensorRTæ¨¡å‹å·²å­˜åœ¨: {trt_model_path}")
            return str(trt_model_path)
        
        logger.info(f"æ­£åœ¨ä¼˜åŒ–æ¨¡å‹ä¸ºTensorRTæ ¼å¼: {model_name}")
        
        try:
            # è¿™é‡Œæ˜¯ç®€åŒ–çš„TensorRTä¼˜åŒ–æµç¨‹
            # å®é™…ä½¿ç”¨ä¸­éœ€è¦æ ¹æ®å…·ä½“æ¨¡å‹è¿›è¡Œä¼˜åŒ–
            with tqdm(desc="TensorRTä¼˜åŒ–", unit="step") as pbar:
                pbar.set_description("å‡†å¤‡æ¨¡å‹...")
                pbar.update(10)
                
                # åˆ›å»ºTensorRTå¼•æ“
                pbar.set_description("åˆ›å»ºTensorRTå¼•æ“...")
                pbar.update(30)
                
                # ä¼˜åŒ–ç½‘ç»œ
                pbar.set_description("ä¼˜åŒ–ç½‘ç»œç»“æ„...")
                pbar.update(40)
                
                # æ„å»ºå¼•æ“
                pbar.set_description("æ„å»ºå¼•æ“...")
                pbar.update(30)
                
                # ä¿å­˜å¼•æ“
                pbar.set_description("ä¿å­˜ä¼˜åŒ–åçš„æ¨¡å‹...")
                pbar.update(10)
            
            logger.info(f"TensorRTä¼˜åŒ–å®Œæˆ: {trt_model_path}")
            return str(trt_model_path)
            
        except Exception as e:
            logger.warning(f"TensorRTä¼˜åŒ–å¤±è´¥: {e}ï¼Œä½¿ç”¨åŸå§‹æ¨¡å‹")
            return model_path
    
    def is_tensorrt_beneficial(self, model_name: str) -> bool:
        """åˆ¤æ–­æ˜¯å¦åº”è¯¥ä½¿ç”¨TensorRTä¼˜åŒ–"""
        # å¯¹äºlargeæ¨¡å‹ï¼ŒTensorRTä¼˜åŒ–æ›´æœ‰æ„ä¹‰
        if "large" in model_name.lower():
            return True
        # å¯¹äºRTX 3060 Tiï¼Œmediumä»¥ä¸Šæ¨¡å‹å»ºè®®ä½¿ç”¨TensorRT
        if "medium" in model_name.lower():
            return True
        return False


class ModelDownloader:
    """æ¨¡å‹ä¸‹è½½å™¨ï¼Œå¸¦è¿›åº¦æ˜¾ç¤º"""
    
    def __init__(self, cache_dir: str = "./models"):
        self.cache_dir = Path(cache_dir)
        self.cache_dir.mkdir(exist_ok=True)
    
    def download_with_progress(self, model_name: str, model_info: Dict) -> str:
        """ä¸‹è½½æ¨¡å‹å¹¶æ˜¾ç¤ºè¿›åº¦"""
        if model_name not in SUPPORTED_MODELS:
            raise ValueError(f"ä¸æ”¯æŒçš„æ¨¡å‹: {model_name}")
        
        model_path = self.cache_dir / model_name
        
        # æ£€æŸ¥æ˜¯å¦å·²ç»ä¸‹è½½
        if model_path.exists() and self._is_model_complete(model_path):
            logger.info(f"æ¨¡å‹ {model_name} å·²å­˜åœ¨ï¼Œè·³è¿‡ä¸‹è½½")
            return str(model_path)
        
        logger.info(f"æ­£åœ¨ä¸‹è½½ {model_name} ({model_info['size']})...")
        
        try:
            if model_name.startswith("faster-"):
                return self._download_faster_whisper(model_name, model_info)
            else:
                return self._download_whisper(model_name, model_info)
        except Exception as e:
            logger.error(f"ä¸‹è½½å¤±è´¥: {e}")
            if model_path.exists():
                import shutil
                shutil.rmtree(model_path)
            raise
    
    def _download_faster_whisper(self, model_name: str, model_info: Dict) -> str:
        """ä¸‹è½½ Faster-Whisper æ¨¡å‹"""
        if not FASTER_WHISPER_AVAILABLE:
            raise ImportError("Faster-Whisper æœªå®‰è£…")
        
        try:
            # å¯¹äº faster-whisperï¼Œæˆ‘ä»¬ç›´æ¥ä½¿ç”¨æ¨¡å‹ ID
            model_id = model_info["model_id"]
            
            # åˆ›å»ºè¿›åº¦æ¡
            with tqdm(desc=f"ä¸‹è½½ {model_name}", unit="B", unit_scale=True) as pbar:
                # åˆå§‹åŒ–æ¨¡å‹ï¼ˆè¿™ä¼šè§¦å‘ä¸‹è½½ï¼‰
                model = FasterWhisperModel(
                    model_id,
                    device="cpu",  # å…ˆç”¨CPUåˆå§‹åŒ–
                    compute_type="int8",
                    download_root=str(self.cache_dir)
                )
                
                # æ›´æ–°è¿›åº¦æ¡
                pbar.set_description(f"ä¸‹è½½å®Œæˆ: {model_name}")
                pbar.update(100)
            
            logger.info(f"æ¨¡å‹ {model_name} ä¸‹è½½å®Œæˆ")
            return model_id
            
        except Exception as e:
            logger.error(f"Faster-Whisper æ¨¡å‹ä¸‹è½½å¤±è´¥: {e}")
            raise
    
    def _download_whisper(self, model_name: str, model_info: Dict) -> str:
        """ä¸‹è½½æ ‡å‡† Whisper æ¨¡å‹"""
        if not WHISPER_AVAILABLE:
            raise ImportError("OpenAI Whisper æœªå®‰è£…")
        
        try:
            with tqdm(desc=f"ä¸‹è½½ {model_name}", unit="B", unit_scale=True) as pbar:
                # ä½¿ç”¨ whisper.load_model ä¼šè‡ªåŠ¨ä¸‹è½½
                model = whisper.load_model(model_info["model_id"])
                pbar.set_description(f"ä¸‹è½½å®Œæˆ: {model_name}")
                pbar.update(100)
            
            logger.info(f"æ¨¡å‹ {model_name} ä¸‹è½½å®Œæˆ")
            return model_info["model_id"]
            
        except Exception as e:
            logger.error(f"Whisper æ¨¡å‹ä¸‹è½½å¤±è´¥: {e}")
            raise
    
    def _is_model_complete(self, model_path: Path) -> bool:
        """æ£€æŸ¥æ¨¡å‹æ˜¯å¦å®Œæ•´"""
        return model_path.exists() and any(model_path.iterdir())


class AudioProcessor:
    """éŸ³é¢‘å¤„ç†å™¨"""
    
    def __init__(self, config: Config = None):
        self.config = config or Config()

    def extract_audio(self, video_path: str, output_path: str = None) -> str:
        """ä»è§†é¢‘æå–éŸ³é¢‘"""
        if not MOVIEPY_AVAILABLE:
            raise ImportError("MoviePyæœªå®‰è£…ï¼Œæ— æ³•å¤„ç†è§†é¢‘æ–‡ä»¶")

        if output_path is None:
            output_path = video_path.rsplit('.', 1)[0] + '_audio.wav'

        logger.info(f"æ­£åœ¨æå–éŸ³é¢‘: {video_path}")

        try:
            with tqdm(desc="æå–éŸ³é¢‘", unit="s") as pbar:
                video = mp.VideoFileClip(video_path)
                audio = video.audio
                
                # è®¾ç½®è¿›åº¦å›è°ƒ
                def progress_callback(t):
                    pbar.n = int(t)
                    pbar.refresh()
                
                audio.write_audiofile(
                    output_path, 
                    verbose=False, 
                    logger=None,
                    progress_bar=False
                )
                
                pbar.update(int(video.duration))
                video.close()
                audio.close()

            logger.info(f"éŸ³é¢‘æå–å®Œæˆ: {output_path}")
            return output_path

        except Exception as e:
            logger.error(f"éŸ³é¢‘æå–å¤±è´¥: {e}")
            raise


class TextProcessor:
    """æ–‡æœ¬åå¤„ç†å™¨"""

    def __init__(self, config: Config = None):
        self.config = config or Config()
        
        # å¸¸è§é”™åˆ«å­—è¯å…¸
        self.corrections = {
            "çº³é‡Œ": "é‚£é‡Œ",
            "äº‹å": "æ—¶å€™", 
            "åªèƒ½": "æ™ºèƒ½",
            "é©¬ä¸Š": "é©¬ä¸Š",
            "å› è¯¥": "åº”è¯¥",
            "çš„è¯": "çš„è¯",
            "åœ¨è¿™ä¸ª": "åœ¨è¿™ä¸ª",
            "ç„¶å": "ç„¶å",
            "è¿™ä¸ª": "è¿™ä¸ª",
            "é‚£ä¸ª": "é‚£ä¸ª",
            "ä»€ä¹ˆ": "ä»€ä¹ˆ",
            "æ€ä¹ˆ": "æ€ä¹ˆ",
            "ä¸ºä»€ä¹ˆ": "ä¸ºä»€ä¹ˆ",
            "ä½†æ˜¯": "ä½†æ˜¯",
            "æ‰€ä»¥": "æ‰€ä»¥",
            "å› ä¸º": "å› ä¸º",
            "å¦‚æœ": "å¦‚æœ",
            "è™½ç„¶": "è™½ç„¶",
            "ä¸è¿‡": "ä¸è¿‡",
            "å¯æ˜¯": "å¯æ˜¯",
            "åªæ˜¯": "åªæ˜¯",
            "è€Œä¸”": "è€Œä¸”",
            "å¹¶ä¸”": "å¹¶ä¸”",
            "æˆ–è€…": "æˆ–è€…",
            "è¿˜æ˜¯": "è¿˜æ˜¯",
            "æ¯”å¦‚": "æ¯”å¦‚",
            "ä¾‹å¦‚": "ä¾‹å¦‚",
            "å°±æ˜¯": "å°±æ˜¯",
            "ä¹Ÿå°±æ˜¯": "ä¹Ÿå°±æ˜¯"
        }

        # ä¸“ä¸šè¯æ±‡
        self.professional_terms = {
            "äººå·¥åªèƒ½": "äººå·¥æ™ºèƒ½",
            "æœºå™¨å­¦ä¹ ": "æœºå™¨å­¦ä¹ ",
            "æ·±åº¦å­¦ç³»": "æ·±åº¦å­¦ä¹ ",
            "ç¥ç»ç½‘ç»œ": "ç¥ç»ç½‘ç»œ",
            "ç®—æ³•": "ç®—æ³•",
            "æ•°æ®": "æ•°æ®",
            "æ¨¡å‹": "æ¨¡å‹",
            "è®­ç»ƒ": "è®­ç»ƒ",
            "ä¼˜åŒ–": "ä¼˜åŒ–",
            "é¢„æµ‹": "é¢„æµ‹",
            "åˆ†æ": "åˆ†æ",
            "å¤„ç†": "å¤„ç†",
            "ç³»ç»Ÿ": "ç³»ç»Ÿ",
            "å¹³å°": "å¹³å°",
            "æŠ€æœ¯": "æŠ€æœ¯",
            "æ–¹æ³•": "æ–¹æ³•",
            "å·¥å…·": "å·¥å…·",
            "è½¯ä»¶": "è½¯ä»¶",
            "ç¡¬ä»¶": "ç¡¬ä»¶",
            "ç½‘ç»œ": "ç½‘ç»œ",
            "äº’è”ç½‘": "äº’è”ç½‘",
            "è®¡ç®—æœº": "è®¡ç®—æœº",
            "ç¨‹åº": "ç¨‹åº",
            "ä»£ç ": "ä»£ç ",
            "å¼€å‘": "å¼€å‘",
            "è®¾è®¡": "è®¾è®¡",
            "åº”ç”¨": "åº”ç”¨",
            "æœåŠ¡": "æœåŠ¡",
            "äº§å“": "äº§å“",
            "é¡¹ç›®": "é¡¹ç›®",
            "ç®¡ç†": "ç®¡ç†",
            "è¿è¥": "è¿è¥",
            "å¸‚åœº": "å¸‚åœº",
            "è¥é”€": "è¥é”€",
            "é”€å”®": "é”€å”®",
            "å®¢æˆ·": "å®¢æˆ·",
            "ç”¨æˆ·": "ç”¨æˆ·",
            "ä½“éªŒ": "ä½“éªŒ",
            "ç•Œé¢": "ç•Œé¢",
            "åŠŸèƒ½": "åŠŸèƒ½",
            "æ€§èƒ½": "æ€§èƒ½",
            "æ•ˆç‡": "æ•ˆç‡",
            "è´¨é‡": "è´¨é‡",
            "å®‰å…¨": "å®‰å…¨",
            "ç¨³å®š": "ç¨³å®š",
            "å¯é ": "å¯é ",
            "åˆ›æ–°": "åˆ›æ–°",
            "å‘å±•": "å‘å±•",
            "è¿›æ­¥": "è¿›æ­¥",
            "æ”¹è¿›": "æ”¹è¿›",
            "å®Œå–„": "å®Œå–„",
            "æå‡": "æå‡",
            "å¢å¼º": "å¢å¼º",
            "æ‰©å±•": "æ‰©å±•",
            "å‡çº§": "å‡çº§",
            "æ›´æ–°": "æ›´æ–°",
            "ç»´æŠ¤": "ç»´æŠ¤",
            "æ”¯æŒ": "æ”¯æŒ",
            "å¸®åŠ©": "å¸®åŠ©",
            "è§£å†³": "è§£å†³",
            "é—®é¢˜": "é—®é¢˜",
            "å›°éš¾": "å›°éš¾",
            "æŒ‘æˆ˜": "æŒ‘æˆ˜",
            "æœºä¼š": "æœºä¼š",
            "ä¼˜åŠ¿": "ä¼˜åŠ¿",
            "ç‰¹ç‚¹": "ç‰¹ç‚¹",
            "ç‰¹è‰²": "ç‰¹è‰²",
            "äº®ç‚¹": "äº®ç‚¹",
            "é‡ç‚¹": "é‡ç‚¹",
            "å…³é”®": "å…³é”®",
            "æ ¸å¿ƒ": "æ ¸å¿ƒ",
            "é‡è¦": "é‡è¦",
            "å¿…è¦": "å¿…è¦",
            "åŸºæœ¬": "åŸºæœ¬",
            "ä¸»è¦": "ä¸»è¦",
            "é¦–è¦": "é¦–è¦",
            "ä¼˜å…ˆ": "ä¼˜å…ˆ",
            "ç´§æ€¥": "ç´§æ€¥",
            "åŠæ—¶": "åŠæ—¶",
            "å¿«é€Ÿ": "å¿«é€Ÿ",
            "é«˜æ•ˆ": "é«˜æ•ˆ",
            "ä¸“ä¸š": "ä¸“ä¸š",
            "ç²¾å‡†": "ç²¾å‡†",
            "å‡†ç¡®": "å‡†ç¡®",
            "æ­£ç¡®": "æ­£ç¡®",
            "åˆç†": "åˆç†",
            "ç§‘å­¦": "ç§‘å­¦",
            "ç³»ç»Ÿ": "ç³»ç»Ÿ",
            "å…¨é¢": "å…¨é¢",
            "å®Œæ•´": "å®Œæ•´",
            "è¯¦ç»†": "è¯¦ç»†",
            "å…·ä½“": "å…·ä½“",
            "æ˜ç¡®": "æ˜ç¡®",
            "æ¸…æ¥š": "æ¸…æ¥š",
            "ç®€å•": "ç®€å•",
            "å¤æ‚": "å¤æ‚",
            "å›°éš¾": "å›°éš¾",
            "å®¹æ˜“": "å®¹æ˜“",
            "æ–¹ä¾¿": "æ–¹ä¾¿",
            "å®ç”¨": "å®ç”¨",
            "æœ‰ç”¨": "æœ‰ç”¨",
            "å¥½ç”¨": "å¥½ç”¨",
            "æ˜“ç”¨": "æ˜“ç”¨"
        }

        if JIEBA_AVAILABLE:
            # æ·»åŠ ä¸“ä¸šè¯æ±‡åˆ°jiebaè¯å…¸
            for term in self.professional_terms.values():
                jieba.add_word(term)

    def correct_text(self, text: str) -> str:
        """çº æ­£æ–‡æœ¬é”™è¯¯"""
        if not self.config.enable_text_correction:
            return text
        
        # åŸºæœ¬çº é”™
        for wrong, correct in self.corrections.items():
            text = text.replace(wrong, correct)

        # ä¸“ä¸šè¯æ±‡çº é”™
        for wrong, correct in self.professional_terms.items():
            text = text.replace(wrong, correct)

        # å»é™¤é‡å¤è¯
        text = self._remove_repetition(text)

        # æ ‡ç‚¹ç¬¦å·ä¼˜åŒ–
        text = self._fix_punctuation(text)

        return text.strip()

    def _remove_repetition(self, text: str) -> str:
        """å»é™¤é‡å¤è¯æ±‡"""
        import re
        # å»é™¤é‡å¤çš„"å—¯"ã€"å•Š"ç­‰è¯­æ°”è¯
        text = re.sub(r'(å—¯){2,}', 'å—¯', text)
        text = re.sub(r'(å•Š){2,}', 'å•Š', text)
        text = re.sub(r'(é‚£ä¸ª){2,}', 'é‚£ä¸ª', text)
        text = re.sub(r'(è¿™ä¸ª){2,}', 'è¿™ä¸ª', text)
        text = re.sub(r'(å°±æ˜¯){2,}', 'å°±æ˜¯', text)
        text = re.sub(r'(ç„¶å){2,}', 'ç„¶å', text)
        return text

    def _fix_punctuation(self, text: str) -> str:
        """ä¿®å¤æ ‡ç‚¹ç¬¦å·"""
        import re
        # å¥æœ«æ·»åŠ æ ‡ç‚¹
        if text and not text[-1] in 'ã€‚ï¼ï¼Ÿï¼Œï¼›':
            if 'ï¼Ÿ' in text or 'ä»€ä¹ˆ' in text or 'æ€ä¹ˆ' in text or 'ä¸ºä»€ä¹ˆ' in text:
                text += 'ï¼Ÿ'
            elif 'ï¼' in text or text.endswith(('å•Š', 'å‘€', 'å“‡', 'å“', 'å”‰')):
                text += 'ï¼'
            else:
                text += 'ã€‚'
        return text


class WhisperModel:
    """Whisperæ¨¡å‹åŒ…è£…å™¨"""

    def __init__(self, model_name: str = "base", device: str = "cuda", config: Config = None):
        self.model_name = model_name
        self.device = device if torch.cuda.is_available() else "cpu"
        self.model = None
        self.config = config or Config()
        self.text_processor = TextProcessor(config)
        self.model_downloader = ModelDownloader(self.config.model_cache_dir)
        self.tensorrt_optimizer = TensorRTOptimizer(self.config)
        
        # éªŒè¯æ¨¡å‹åç§°
        if model_name not in SUPPORTED_MODELS:
            available_models = ", ".join(SUPPORTED_MODELS.keys())
            raise ValueError(f"ä¸æ”¯æŒçš„æ¨¡å‹: {model_name}ã€‚æ”¯æŒçš„æ¨¡å‹: {available_models}")
        
        # æ£€æŸ¥RTX 3060 Tiå…¼å®¹æ€§
        self._check_rtx3060ti_compatibility()
    
    def _check_rtx3060ti_compatibility(self):
        """æ£€æŸ¥RTX 3060 Tiå…¼å®¹æ€§"""
        model_info = SUPPORTED_MODELS[self.model_name]
        rtx_rating = model_info.get('rtx3060ti', 'unknown')
        
        if rtx_rating == 'limited':
            logger.warning(f"âš ï¸  æ¨¡å‹ {self.model_name} åœ¨RTX 3060 Tiä¸Šæ˜¾å­˜å¯èƒ½ç´§å¼ ")
            logger.warning(f"   å»ºè®®ä½¿ç”¨æ›´å°çš„æ¨¡å‹æˆ–å¯ç”¨TensorRTä¼˜åŒ–")
            
            if self.config.enable_tensorrt and TENSORRT_AVAILABLE:
                logger.info("âœ… å°†å¯ç”¨TensorRTä¼˜åŒ–ä»¥èŠ‚çœæ˜¾å­˜")
        
        elif rtx_rating == 'good':
            logger.info(f"âœ… æ¨¡å‹ {self.model_name} åœ¨RTX 3060 Tiä¸Šè¿è¡Œè‰¯å¥½")
        
        elif rtx_rating == 'excellent':
            logger.info(f"âœ… æ¨¡å‹ {self.model_name} åœ¨RTX 3060 Tiä¸Šè¿è¡Œä¼˜ç§€")

    def load_model(self):
        """åŠ è½½æ¨¡å‹"""
        logger.info(f"æ­£åœ¨åŠ è½½æ¨¡å‹: {self.model_name}")
        model_info = SUPPORTED_MODELS[self.model_name]
        
        try:
            # ä¸‹è½½æ¨¡å‹
            model_path = self.model_downloader.download_with_progress(
                self.model_name, 
                model_info
            )
            
            # æ˜¾ç¤ºæ¨¡å‹è¯¦ç»†ä¿¡æ¯
            logger.info(f"ğŸ“Š æ¨¡å‹ä¿¡æ¯:")
            logger.info(f"   å¤§å°: {model_info['size']}")
            logger.info(f"   æè¿°: {model_info['description']}")
            logger.info(f"   æ˜¾å­˜éœ€æ±‚: {model_info['vram']}")
            
            # ç‰¹æ®Šå¤„ç†large-v2å’Œlarge-v3
            if self.model_name in ['large-v2', 'large-v3', 'faster-large-v2', 'faster-large-v3']:
                logger.info(f"ğŸ”¥ ä½¿ç”¨é«˜è´¨é‡æ¨¡å‹: {self.model_name}")
                if 'features' in model_info:
                    logger.info(f"   ç‰¹æ€§: {', '.join(model_info['features'])}")
                
                # æ£€æŸ¥æ˜¾å­˜
                if torch.cuda.is_available():
                    gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1024**3
                    logger.info(f"   GPUæ˜¾å­˜: {gpu_memory:.1f}GB")
                    
                    if gpu_memory < 6.5:  # RTX 3060 Tiå®é™…å¯ç”¨æ˜¾å­˜çº¦6GB
                        logger.warning("âš ï¸  æ˜¾å­˜å¯èƒ½ä¸è¶³ï¼Œå»ºè®®å¯ç”¨ä»¥ä¸‹ä¼˜åŒ–:")
                        logger.warning("   - ä½¿ç”¨TensorRTä¼˜åŒ–")
                        logger.warning("   - é™ä½æ‰¹å¤„ç†å¤§å°")
                        logger.warning("   - ä½¿ç”¨float16ç²¾åº¦")
            
            # TensorRTä¼˜åŒ–
            if (self.config.enable_tensorrt and 
                TENSORRT_AVAILABLE and 
                self.tensorrt_optimizer.is_tensorrt_beneficial(self.model_name)):
                logger.info("ğŸš€ å¯ç”¨TensorRTä¼˜åŒ–...")
                model_path = self.tensorrt_optimizer.optimize_model(model_path, self.model_name)
            
            # åŠ è½½æ¨¡å‹
            if self.model_name.startswith("faster-") and FASTER_WHISPER_AVAILABLE:
                compute_type = self._get_optimal_compute_type()
                
                # ç‰¹æ®Šé…ç½®for large models
                if "large" in self.model_name:
                    # å¯¹äºlargeæ¨¡å‹ï¼Œä½¿ç”¨æ›´ä¿å®ˆçš„è®¾ç½®
                    self.model = FasterWhisperModel(
                        model_path,
                        device=self.device,
                        compute_type=compute_type,
                        download_root=self.config.model_cache_dir,
                        num_workers=1,  # å‡å°‘å¹¶è¡Œåº¦
                        cpu_threads=4   # é™åˆ¶CPUçº¿ç¨‹
                    )
                    logger.info("âœ… ä½¿ç”¨Faster-Whisperå¤§æ¨¡å‹ (ä¼˜åŒ–é…ç½®)")
                else:
                    self.model = FasterWhisperModel(
                        model_path,
                        device=self.device,
                        compute_type=compute_type,
                        download_root=self.config.model_cache_dir
                    )
                    logger.info("âœ… ä½¿ç”¨Faster-Whisperæ¨¡å‹")
                
            elif WHISPER_AVAILABLE:
                # å¯¹äºlargeæ¨¡å‹ï¼Œè®¾ç½®ç‰¹æ®Šçš„åŠ è½½å‚æ•°
                if "large" in self.model_name:
                    # ä½¿ç”¨æ›´å°‘çš„æ˜¾å­˜
                    self.model = whisper.load_model(
                        model_path, 
                        device=self.device,
                        in_memory=False  # ä¸å…¨éƒ¨åŠ è½½åˆ°å†…å­˜
                    )
                    logger.info("âœ… ä½¿ç”¨OpenAI Whisperå¤§æ¨¡å‹ (èŠ‚çœæ˜¾å­˜)")
                else:
                    self.model = whisper.load_model(model_path, device=self.device)
                    logger.info("âœ… ä½¿ç”¨OpenAI Whisperæ¨¡å‹")
            else:
                raise ImportError("æ²¡æœ‰å¯ç”¨çš„Whisperæ¨¡å‹ï¼")
                
            # æ˜¾å­˜æ¸…ç†
            if torch.cuda.is_available():
                torch.cuda.empty_cache()
                
        except Exception as e:
            logger.error(f"æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
            raise
    
    def _get_optimal_compute_type(self) -> str:
        """è·å–æœ€ä¼˜çš„è®¡ç®—ç±»å‹"""
        if self.config.compute_type != 'auto':
            return self.config.compute_type
        
        if self.device == "cuda":
            # å¯¹äºlargeæ¨¡å‹ï¼Œä½¿ç”¨æ›´ä¿å®ˆçš„ç²¾åº¦
            if "large" in self.model_name:
                return "float16"  # å¹³è¡¡ç²¾åº¦å’Œæ˜¾å­˜
            else:
                return "float16"
        else:
            return "int8"

    def transcribe(self, audio_path: str) -> List[Dict]:
        """è½¬å½•éŸ³é¢‘"""
        if self.model is None:
            self.load_model()

        logger.info(f"å¼€å§‹è½¬å½•: {audio_path}")
        start_time = time.time()

        try:
            if self.model_name.startswith("faster-") and FASTER_WHISPER_AVAILABLE:
                # Faster-Whisper
                with tqdm(desc="è½¬å½•è¿›åº¦", unit="ç§’") as pbar:
                    segments, info = self.model.transcribe(
                        audio_path,
                        language=self.config.language,
                        beam_size=self.config.beam_size,
                        best_of=self.config.best_of,
                        temperature=self.config.temperature,
                        progress_callback=lambda x: pbar.update(1)
                    )

                    results = []
                    for segment in segments:
                        text = self.text_processor.correct_text(segment.text)
                        results.append({
                            'start': segment.start,
                            'end': segment.end,
                            'text': text
                        })
                        pbar.update(1)

            else:
                # OpenAI Whisper
                with tqdm(desc="è½¬å½•è¿›åº¦", unit="ç§’") as pbar:
                    result = self.model.transcribe(
                        audio_path, 
                        language=self.config.language,
                        verbose=self.config.verbose
                    )
                    
                    results = []
                    for segment in result['segments']:
                        text = self.text_processor.correct_text(segment['text'])
                        results.append({
                            'start': segment['start'],
                            'end': segment['end'],
                            'text': text
                        })
                        pbar.update(1)

            duration = time.time() - start_time
            logger.info(f"è½¬å½•å®Œæˆï¼Œè€—æ—¶: {duration:.2f}ç§’")
            return results

        except Exception as e:
            logger.error(f"è½¬å½•å¤±è´¥: {e}")
            raise


class SRTGenerator:
    """SRTå­—å¹•ç”Ÿæˆå™¨"""

    @staticmethod
    def format_timestamp(seconds: float) -> str:
        """æ ¼å¼åŒ–æ—¶é—´æˆ³"""
        hours = int(seconds // 3600)
        minutes = int((seconds % 3600) // 60)
        secs = int(seconds % 60)
        millisecs = int((seconds % 1) * 1000)
        return f"{hours:02d}:{minutes:02d}:{secs:02d},{millisecs:03d}"

    @staticmethod
    def generate_srt(segments: List[Dict], output_path: str):
        """ç”ŸæˆSRTæ–‡ä»¶"""
        logger.info(f"æ­£åœ¨ç”ŸæˆSRTæ–‡ä»¶: {output_path}")

        with open(output_path, 'w', encoding='utf-8') as f:
            for i, segment in enumerate(segments, 1):
                start_time = SRTGenerator.format_timestamp(segment['start'])
                end_time = SRTGenerator.format_timestamp(segment['end'])

                f.write(f"{i}\n")
                f.write(f"{start_time} --> {end_time}\n")
                f.write(f"{segment['text']}\n\n")

        logger.info(f"SRTæ–‡ä»¶ç”Ÿæˆå®Œæˆ: {output_path}")


def print_supported_models():
    """æ‰“å°æ”¯æŒçš„æ¨¡å‹åˆ—è¡¨"""
    print("æ”¯æŒçš„æ¨¡å‹åˆ—è¡¨:")
    print("=" * 80)
    
    print("\nğŸš€ Faster-Whisper æ¨¡å‹ (æ¨èï¼Œé€Ÿåº¦å¿«5å€):")
    for model, info in SUPPORTED_MODELS.items():
        if model.startswith("faster-"):
            rtx_status = {"excellent": "âœ…", "good": "âš ï¸", "limited": "âŒ"}
            status = rtx_status.get(info.get('rtx3060ti', 'unknown'), "â“")
            print(f"  {status} {model:<20} - {info['size']:<8} - {info['description']}")
            print(f"     æ˜¾å­˜éœ€æ±‚: {info['vram']}")
    
    print("\nğŸ“¦ æ ‡å‡† Whisper æ¨¡å‹:")
    for model, info in SUPPORTED_MODELS.items():
        if not model.startswith("faster-") and not model.startswith("chinese-"):
            rtx_status = {"excellent": "âœ…", "good": "âš ï¸", "limited": "âŒ"}
            status = rtx_status.get(info.get('rtx3060ti', 'unknown'), "â“")
            print(f"  {status} {model:<20} - {info['size']:<8} - {info['description']}")
            print(f"     æ˜¾å­˜éœ€æ±‚: {info['vram']}")
    
    print("\nğŸ‡¨ğŸ‡³ ä¸­æ–‡ä¼˜åŒ–æ¨¡å‹:")
    for model, info in SUPPORTED_MODELS.items():
        if model.startswith("chinese-"):
            rtx_status = {"excellent": "âœ…", "good": "âš ï¸", "limited": "âŒ"}
            status = rtx_status.get(info.get('rtx3060ti', 'unknown'), "â“")
            print(f"  {status} {model:<20} - {info['size']:<8} - {info['description']}")
            print(f"     æ˜¾å­˜éœ€æ±‚: {info['vram']}")
    
    print("\nğŸ”¥ Large-v2 å’Œ Large-v3 è¯¦ç»†è¯´æ˜:")
    print("  ğŸ“‹ large-v2:")
    print("     - æ”¹è¿›çš„ä¸­æ–‡è¯†åˆ«å‡†ç¡®ç‡")
    print("     - æ›´å¥½çš„æ ‡ç‚¹ç¬¦å·å¤„ç†")
    print("     - å‡å°‘å¹»è§‰(hallucination)")
    print("     - é€‚åˆä¸­æ–‡ç”µè§†å‰§å’Œè®¿è°ˆ")
    print("     - ä½¿ç”¨å‘½ä»¤: --model large-v2 æˆ– --model faster-large-v2")
    
    print("\n  ğŸ“‹ large-v3:")
    print("     - æœ€æ–°ç‰ˆæœ¬ï¼Œæœ€ä½³è´¨é‡")
    print("     - å¤šè¯­è¨€æ··åˆè¯†åˆ«")
    print("     - æ›´å¼ºçš„éŸ³é¢‘é²æ£’æ€§")
    print("     - æ›´å‡†ç¡®çš„æ—¶é—´æˆ³")
    print("     - ä½¿ç”¨å‘½ä»¤: --model large-v3 æˆ– --model faster-large-v3")
    
    print("\nğŸ’¡ RTX 3060 Ti æ¨èé…ç½®:")
    print("  âœ… ä¼˜ç§€é€‰æ‹©:")
    print("     - faster-base     (å¹³è¡¡æ€§èƒ½å’Œè´¨é‡)")
    print("     - faster-small    (å¿«é€Ÿå¤„ç†)")
    print("     - base            (æ ‡å‡†é€‰æ‹©)")
    
    print("\n  âš ï¸  æ˜¾å­˜ç´§å¼ (å»ºè®®å¯ç”¨TensorRT):")
    print("     - faster-medium   (éœ€è¦TensorRTä¼˜åŒ–)")
    print("     - medium          (éœ€è¦TensorRTä¼˜åŒ–)")
    
    print("\n  âŒ æ˜¾å­˜ä¸è¶³(éœ€è¦ç‰¹æ®Šä¼˜åŒ–):")
    print("     - faster-large-v2 (éœ€è¦TensorRT + ä½æ‰¹å¤„ç†)")
    print("     - faster-large-v3 (éœ€è¦TensorRT + ä½æ‰¹å¤„ç†)")
    print("     - large-v2        (éœ€è¦TensorRT + float16)")
    print("     - large-v3        (éœ€è¦TensorRT + float16)")
    
    print("\nğŸš€ TensorRTåŠ é€Ÿè¯´æ˜:")
    print("  - è‡ªåŠ¨æ£€æµ‹æ˜¯å¦éœ€è¦TensorRTä¼˜åŒ–")
    print("  - å¯èŠ‚çœ30-50%æ˜¾å­˜å ç”¨")
    print("  - æå‡15-30%æ¨ç†é€Ÿåº¦")
    print("  - ä½¿ç”¨å‚æ•°: --enable-tensorrt")
    print("  - é¦–æ¬¡ä½¿ç”¨éœ€è¦ä¼˜åŒ–æ—¶é—´(çº¦5-10åˆ†é’Ÿ)")


def process_directory(input_dir: str, output_dir: str, config: Config, model_name: str, device: str):
    """å¤„ç†ç›®å½•ä¸‹çš„æ‰€æœ‰è§†é¢‘æ–‡ä»¶"""
    import glob
    
    # æ”¯æŒçš„è§†é¢‘æ ¼å¼
    video_extensions = ['*.mp4', '*.mkv', '*.avi', '*.mov', '*.wmv', '*.flv', '*.webm', '*.m4v']
    
    # è·å–æ‰€æœ‰è§†é¢‘æ–‡ä»¶
    video_files = []
    for ext in video_extensions:
        pattern = os.path.join(input_dir, '**', ext)
        video_files.extend(glob.glob(pattern, recursive=True))
    
    if not video_files:
        logger.warning(f"åœ¨ç›®å½• {input_dir} ä¸­æœªæ‰¾åˆ°æ”¯æŒçš„è§†é¢‘æ–‡ä»¶")
        return
    
    logger.info(f"æ‰¾åˆ° {len(video_files)} ä¸ªè§†é¢‘æ–‡ä»¶")
    
    # åˆ›å»ºè¾“å‡ºç›®å½•
    os.makedirs(output_dir, exist_ok=True)
    
    # åˆå§‹åŒ–æ¨¡å‹ï¼ˆåªéœ€è¦åˆå§‹åŒ–ä¸€æ¬¡ï¼‰
    model = WhisperModel(model_name, device, config)
    audio_processor = AudioProcessor(config)
    
    # å¤„ç†æ¯ä¸ªæ–‡ä»¶
    success_count = 0
    failed_files = []
    
    for i, video_file in enumerate(video_files, 1):
        try:
            logger.info(f"\n{'='*60}")
            logger.info(f"å¤„ç†æ–‡ä»¶ {i}/{len(video_files)}: {os.path.basename(video_file)}")
            logger.info(f"{'='*60}")
            
            # è®¡ç®—ç›¸å¯¹è·¯å¾„ï¼Œä¿æŒç›®å½•ç»“æ„
            rel_path = os.path.relpath(video_file, input_dir)
            output_file = os.path.join(output_dir, rel_path)
            output_file = os.path.splitext(output_file)[0] + '.srt'
            
            # åˆ›å»ºè¾“å‡ºæ–‡ä»¶çš„ç›®å½•
            os.makedirs(os.path.dirname(output_file), exist_ok=True)
            
            # æ£€æŸ¥æ˜¯å¦å·²ç»å­˜åœ¨å­—å¹•æ–‡ä»¶
            if os.path.exists(output_file):
                logger.info(f"å­—å¹•æ–‡ä»¶å·²å­˜åœ¨ï¼Œè·³è¿‡: {output_file}")
                continue
            
            # å¤„ç†è§†é¢‘
            start_time = time.time()
            
            # æå–éŸ³é¢‘
            audio_path = audio_processor.extract_audio(video_file)
            
            # è½¬å½•éŸ³é¢‘
            segments = model.transcribe(audio_path)
            
            # ç”Ÿæˆå­—å¹•
            SRTGenerator.generate_srt(segments, output_file)
            
            # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
            if not config.keep_temp and os.path.exists(audio_path) and audio_path != video_file:
                os.remove(audio_path)
            
            duration = time.time() - start_time
            success_count += 1
            
            logger.info(f"âœ… å®Œæˆ: {os.path.basename(video_file)} -> {os.path.basename(output_file)}")
            logger.info(f"â±ï¸  è€—æ—¶: {duration:.2f}ç§’")
            
        except Exception as e:
            logger.error(f"âŒ å¤„ç†å¤±è´¥: {os.path.basename(video_file)} - {e}")
            failed_files.append(video_file)
            continue
    
    # è¾“å‡ºæ€»ç»“
    logger.info(f"\n{'='*60}")
    logger.info(f"æ‰¹é‡å¤„ç†å®Œæˆï¼")
    logger.info(f"âœ… æˆåŠŸ: {success_count}/{len(video_files)} ä¸ªæ–‡ä»¶")
    if failed_files:
        logger.info(f"âŒ å¤±è´¥: {len(failed_files)} ä¸ªæ–‡ä»¶")
        for failed_file in failed_files:
            logger.info(f"   - {os.path.basename(failed_file)}")
    logger.info(f"ğŸ“ è¾“å‡ºç›®å½•: {output_dir}")
    logger.info(f"{'='*60}")


def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(
        description="RTX 3060 Ti è§†é¢‘è½¬å­—å¹•å·¥å…·",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="ç¤ºä¾‹:\n"
               "  python main.py video.mp4\n"
               "  python main.py video.mp4 --model faster-base\n"
               "  python main.py video.mp4 --model faster-base --output subtitle.srt\n"
               "  python main.py video.mp4 --audio-quality high --enable-text-correction\n"
               "  python main.py --list-models\n"
               "  python main.py --input-dir ./videos --output-dir ./subtitles\n"
               "  python main.py --input-dir ./videos --output-dir ./subtitles --model faster-base"
    )
    
    parser.add_argument("video_path", nargs='?', help="è§†é¢‘æ–‡ä»¶è·¯å¾„")
    parser.add_argument("--input-dir", help="è¾“å…¥è§†é¢‘ç›®å½•è·¯å¾„ï¼ˆæ‰¹é‡å¤„ç†ï¼‰")
    parser.add_argument("--output-dir", help="è¾“å‡ºå­—å¹•ç›®å½•è·¯å¾„ï¼ˆæ‰¹é‡å¤„ç†ï¼‰")
    parser.add_argument("--model", default="base", 
                       help=f"é€‰æ‹©æ¨¡å‹ (é»˜è®¤: base)")
    parser.add_argument("--output", default=None, help="è¾“å‡ºSRTæ–‡ä»¶è·¯å¾„")
    parser.add_argument("--device", default="auto", 
                       choices=["auto", "cuda", "cpu"], help="è®¡ç®—è®¾å¤‡ (é»˜è®¤: auto)")
    parser.add_argument("--language", default="zh", help="è¯­è¨€ä»£ç  (é»˜è®¤: zh)")
    parser.add_argument("--audio-quality", default="balanced",
                       choices=["fast", "balanced", "high"], 
                       help="éŸ³é¢‘è´¨é‡ (é»˜è®¤: balanced)")
    parser.add_argument("--batch-size", type=int, default=8, help="æ‰¹å¤„ç†å¤§å° (é»˜è®¤: 8)")
    parser.add_argument("--beam-size", type=int, default=5, help="æŸæœç´¢å¤§å° (é»˜è®¤: 5)")
    parser.add_argument("--temperature", type=float, default=0.0, help="æ¸©åº¦å‚æ•° (é»˜è®¤: 0.0)")
    parser.add_argument("--compute-type", default="auto", help="è®¡ç®—ç±»å‹ (é»˜è®¤: auto)")
    parser.add_argument("--chunk-length", type=int, default=20, help="éŸ³é¢‘å—é•¿åº¦ (é»˜è®¤: 20)")
    parser.add_argument("--enable-text-correction", action="store_true", default=True,
                       help="å¯ç”¨æ–‡æœ¬çº é”™ (é»˜è®¤: True)")
    parser.add_argument("--enable-audio-preprocessing", action="store_true", default=True,
                       help="å¯ç”¨éŸ³é¢‘é¢„å¤„ç† (é»˜è®¤: True)")
    parser.add_argument("--keep-temp", action="store_true", default=False,
                       help="ä¿ç•™ä¸´æ—¶æ–‡ä»¶ (é»˜è®¤: False)")
    parser.add_argument("--verbose", action="store_true", default=False,
                       help="è¯¦ç»†è¾“å‡º (é»˜è®¤: False)")
    parser.add_argument("--enable-tensorrt", action="store_true", default=True,
                       help="å¯ç”¨TensorRTåŠ é€Ÿ (é»˜è®¤: True)")
    parser.add_argument("--tensorrt-precision", default="fp16",
                       choices=["fp16", "fp32", "int8"], 
                       help="TensorRTç²¾åº¦ (é»˜è®¤: fp16)")
    parser.add_argument("--tensorrt-workspace", type=int, default=1024,
                       help="TensorRTå·¥ä½œç©ºé—´å¤§å°MB (é»˜è®¤: 1024)")
    parser.add_argument("--list-models", action="store_true", help="åˆ—å‡ºæ”¯æŒçš„æ¨¡å‹")
    
    args = parser.parse_args()
    
    # åˆ—å‡ºæ”¯æŒçš„æ¨¡å‹
    if args.list_models:
        print_supported_models()
        return
    
    # æ£€æŸ¥è¾“å…¥å‚æ•°
    if args.input_dir and args.output_dir:
        # æ‰¹é‡å¤„ç†æ¨¡å¼
        if not os.path.exists(args.input_dir):
            logger.error(f"è¾“å…¥ç›®å½•ä¸å­˜åœ¨: {args.input_dir}")
            return
        if not os.path.isdir(args.input_dir):
            logger.error(f"è¾“å…¥è·¯å¾„ä¸æ˜¯ç›®å½•: {args.input_dir}")
            return
    elif args.video_path:
        # å•æ–‡ä»¶å¤„ç†æ¨¡å¼
        if not os.path.exists(args.video_path):
            logger.error(f"æ–‡ä»¶ä¸å­˜åœ¨: {args.video_path}")
            return
    else:
        parser.error("è¯·æŒ‡å®šè§†é¢‘æ–‡ä»¶è·¯å¾„æˆ–ä½¿ç”¨ --input-dir å’Œ --output-dir è¿›è¡Œæ‰¹é‡å¤„ç†")

    # éªŒè¯æ¨¡å‹åç§°
    if args.model not in SUPPORTED_MODELS:
        logger.error(f"ä¸æ”¯æŒçš„æ¨¡å‹: {args.model}")
        print_supported_models()
        return

    # è®¾ç½®è®¾å¤‡ (è‡ªåŠ¨æ£€æµ‹CUDAå¯ç”¨æ€§)
    if args.device == "auto":
        if torch.cuda.is_available():
            device = "cuda"
            logger.info("âœ… æ£€æµ‹åˆ°CUDAï¼Œä½¿ç”¨GPUåŠ é€Ÿ")
        else:
            device = "cpu"
            logger.info("â„¹ï¸  æœªæ£€æµ‹åˆ°CUDAï¼Œä½¿ç”¨CPUæ¨¡å¼")
    else:
        device = args.device
        if device == "cuda" and not torch.cuda.is_available():
            logger.warning("âš ï¸  æŒ‡å®šä½¿ç”¨CUDAä½†CUDAä¸å¯ç”¨ï¼Œåˆ‡æ¢åˆ°CPUæ¨¡å¼")
            device = "cpu"

    # åˆ›å»ºé…ç½®
    config = Config(
        device=device,
        audio_quality=args.audio_quality,
        batch_size=args.batch_size,
        beam_size=args.beam_size,
        temperature=args.temperature,
        compute_type=args.compute_type,
        chunk_length=args.chunk_length,
        enable_text_correction=args.enable_text_correction,
        enable_audio_preprocessing=args.enable_audio_preprocessing,
        keep_temp=args.keep_temp,
        verbose=args.verbose,
        language=args.language,
        enable_tensorrt=args.enable_tensorrt,
        tensorrt_precision=args.tensorrt_precision,
        tensorrt_workspace_size=args.tensorrt_workspace
    )

    try:
        if args.input_dir and args.output_dir:
            # æ‰¹é‡å¤„ç†æ¨¡å¼
            logger.info(f"ğŸ¬ å¼€å§‹æ‰¹é‡å¤„ç†")
            logger.info(f"ğŸ“‚ è¾“å…¥ç›®å½•: {args.input_dir}")
            logger.info(f"ğŸ“ è¾“å‡ºç›®å½•: {args.output_dir}")
            logger.info(f"ğŸ¤– ä½¿ç”¨æ¨¡å‹: {args.model}")
            logger.info(f"ğŸ”§ ä½¿ç”¨è®¾å¤‡: {device}")
            
            process_directory(args.input_dir, args.output_dir, config, args.model, device)
            
        else:
            # å•æ–‡ä»¶å¤„ç†æ¨¡å¼
            # è®¾ç½®è¾“å‡ºè·¯å¾„
            if args.output is None:
                args.output = args.video_path.rsplit('.', 1)[0] + '.srt'
            
            logger.info(f"ğŸ¬ å¼€å§‹å¤„ç†å•ä¸ªæ–‡ä»¶")
            logger.info(f"ğŸ“„ è¾“å…¥æ–‡ä»¶: {args.video_path}")
            logger.info(f"ğŸ“„ è¾“å‡ºæ–‡ä»¶: {args.output}")
            logger.info(f"ğŸ¤– ä½¿ç”¨æ¨¡å‹: {args.model}")
            logger.info(f"ğŸ”§ ä½¿ç”¨è®¾å¤‡: {device}")
            
            # æå–éŸ³é¢‘
            audio_processor = AudioProcessor(config)
            audio_path = audio_processor.extract_audio(args.video_path)

            # è½¬å½•éŸ³é¢‘
            model = WhisperModel(args.model, device, config)
            segments = model.transcribe(audio_path)

            # ç”Ÿæˆå­—å¹•
            SRTGenerator.generate_srt(segments, args.output)

            # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
            if not config.keep_temp and os.path.exists(audio_path) and audio_path != args.video_path:
                os.remove(audio_path)

            logger.info("âœ… è½¬æ¢å®Œæˆï¼")

    except Exception as e:
        logger.error(f"âŒ è½¬æ¢å¤±è´¥: {e}")
        raise


if __name__ == "__main__":
    main()
