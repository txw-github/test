
#!/usr/bin/env python3
"""
ç»Ÿä¸€æ¨¡å‹ç®¡ç†å™¨ - æ”¯æŒå¤šç§ASRæ¨¡å‹
"""

import os
import json
import time
import logging
from typing import Dict, List, Optional, Any
from dataclasses import dataclass
from pathlib import Path

logger = logging.getLogger(__name__)

@dataclass
class ModelInfo:
    """æ¨¡å‹ä¿¡æ¯"""
    name: str
    model_type: str  # whisper, funasr, fireredasr, sensevoice
    size: str  # tiny, small, base, large
    language: str
    precision: str
    tensorrt_support: bool
    recommended_gpu: List[str]
    min_vram_gb: float
    download_url: Optional[str] = None
    local_path: Optional[str] = None

class ModelRegistry:
    """æ¨¡å‹æ³¨å†Œè¡¨"""
    
    def __init__(self):
        self.models = self._init_models()
    
    def _init_models(self) -> Dict[str, ModelInfo]:
        """åˆå§‹åŒ–æ”¯æŒçš„æ¨¡å‹åˆ—è¡¨"""
        models = {}
        
        # Whisperç³»åˆ—
        whisper_models = [
            ("tiny", 0.5, ["GTX 1060", "RTX 2060"]),
            ("base", 1.0, ["GTX 1660", "RTX 3060"]),
            ("small", 2.0, ["RTX 3060", "RTX 3060 Ti"]),
            ("medium", 5.0, ["RTX 3070", "RTX 4060"]),
            ("large", 10.0, ["RTX 3080", "RTX 4070"])
        ]
        
        for size, vram, gpus in whisper_models:
            models[size] = ModelInfo(
                name=size,
                model_type="whisper",
                size=size,
                language="multilingual",
                precision="fp16",
                tensorrt_support=True,
                recommended_gpu=gpus,
                min_vram_gb=vram
            )
            
            # Faster-Whisperç‰ˆæœ¬
            models[f"faster-{size}"] = ModelInfo(
                name=f"faster-{size}",
                model_type="faster-whisper",
                size=size,
                language="multilingual",
                precision="fp16",
                tensorrt_support=True,
                recommended_gpu=gpus,
                min_vram_gb=vram * 0.7  # Faster-Whisperæ›´çœå†…å­˜
            )
        
        # FunASRç³»åˆ—
        funasr_models = [
            ("funasr-paraformer", "paraformer", 4.0, ["RTX 3060 Ti", "RTX 3070"]),
            ("funasr-conformer", "conformer", 6.0, ["RTX 3070", "RTX 4060"])
        ]
        
        for name, arch, vram, gpus in funasr_models:
            models[name] = ModelInfo(
                name=name,
                model_type="funasr",
                size="base",
                language="chinese",
                precision="fp16",
                tensorrt_support=True,
                recommended_gpu=gpus,
                min_vram_gb=vram
            )
        
        # FireRedASRç³»åˆ—
        firered_models = [
            ("fireredasr-small", "small", 2.0, ["RTX 3060", "RTX 3060 Ti"]),
            ("fireredasr-base", "base", 4.0, ["RTX 3060 Ti", "RTX 3070"]),
            ("fireredasr-large", "large", 8.0, ["RTX 3080", "RTX 4070"])
        ]
        
        for name, size, vram, gpus in firered_models:
            models[name] = ModelInfo(
                name=name,
                model_type="fireredasr",
                size=size,
                language="chinese",
                precision="fp16",
                tensorrt_support=True,
                recommended_gpu=gpus,
                min_vram_gb=vram
            )
        
        # SenseVoiceç³»åˆ—
        sensevoice_models = [
            ("sensevoice-small", "small", 3.0, ["RTX 3060 Ti", "RTX 3070"]),
            ("sensevoice-large", "large", 6.0, ["RTX 3070", "RTX 4060"])
        ]
        
        for name, size, vram, gpus in sensevoice_models:
            models[name] = ModelInfo(
                name=name,
                model_type="sensevoice",
                size=size,
                language="chinese",
                precision="fp16",
                tensorrt_support=True,
                recommended_gpu=gpus,
                min_vram_gb=vram
            )
        
        return models
    
    def get_model_info(self, model_name: str) -> Optional[ModelInfo]:
        """è·å–æ¨¡å‹ä¿¡æ¯"""
        return self.models.get(model_name)
    
    def get_recommended_models(self, gpu_name: str = None, vram_gb: float = None) -> List[ModelInfo]:
        """è·å–æ¨èæ¨¡å‹"""
        if not gpu_name and not vram_gb:
            # è‡ªåŠ¨æ£€æµ‹GPU
            try:
                import torch
                if torch.cuda.is_available():
                    gpu_name = torch.cuda.get_device_name(0)
                    vram_gb = torch.cuda.get_device_properties(0).total_memory / (1024**3)
            except:
                pass
        
        recommended = []
        for model in self.models.values():
            # æ£€æŸ¥GPUå…¼å®¹æ€§
            gpu_compatible = True
            if gpu_name:
                gpu_compatible = any(gpu in gpu_name for gpu in model.recommended_gpu)
            
            # æ£€æŸ¥æ˜¾å­˜è¦æ±‚
            vram_compatible = True
            if vram_gb:
                vram_compatible = model.min_vram_gb <= vram_gb
            
            if gpu_compatible and vram_compatible:
                recommended.append(model)
        
        # æŒ‰æ˜¾å­˜è¦æ±‚æ’åº
        return sorted(recommended, key=lambda x: x.min_vram_gb)
    
    def get_models_by_type(self, model_type: str) -> List[ModelInfo]:
        """æŒ‰ç±»å‹è·å–æ¨¡å‹"""
        return [model for model in self.models.values() if model.model_type == model_type]

class ModelManager:
    """ç»Ÿä¸€æ¨¡å‹ç®¡ç†å™¨"""
    
    def __init__(self, config_path: str = "model_config.json"):
        self.config_path = config_path
        self.registry = ModelRegistry()
        self.config = self._load_config()
        
    def _load_config(self) -> Dict:
        """åŠ è½½é…ç½®"""
        default_config = {
            "default_model": "faster-base",
            "auto_optimize": True,
            "tensorrt_enabled": True,
            "precision": "fp16",
            "cache_models": True,
            "models_path": "./models",
            "preferred_models": {
                "RTX 3060 Ti": ["faster-base", "funasr-paraformer", "fireredasr-base"],
                "RTX 3060": ["faster-base", "small", "fireredasr-small"],
                "RTX 3070": ["faster-large", "funasr-paraformer", "fireredasr-base"],
                "RTX 4060": ["medium", "funasr-conformer", "fireredasr-large"]
            }
        }
        
        if os.path.exists(self.config_path):
            try:
                with open(self.config_path, 'r', encoding='utf-8') as f:
                    config = json.load(f)
                    # åˆå¹¶é»˜è®¤é…ç½®
                    for key, value in default_config.items():
                        if key not in config:
                            config[key] = value
                    return config
            except Exception as e:
                logger.warning(f"é…ç½®æ–‡ä»¶åŠ è½½å¤±è´¥: {e}")
        
        return default_config
    
    def save_config(self):
        """ä¿å­˜é…ç½®"""
        try:
            with open(self.config_path, 'w', encoding='utf-8') as f:
                json.dump(self.config, f, indent=2, ensure_ascii=False)
        except Exception as e:
            logger.error(f"é…ç½®ä¿å­˜å¤±è´¥: {e}")
    
    def get_optimal_model(self, task: str = "transcription", language: str = "zh") -> str:
        """è·å–æœ€ä½³æ¨¡å‹"""
        try:
            import torch
            if not torch.cuda.is_available():
                return "tiny"  # CPUæ¨¡å¼ä½¿ç”¨æœ€å°æ¨¡å‹
            
            gpu_name = torch.cuda.get_device_name(0)
            vram_gb = torch.cuda.get_device_properties(0).total_memory / (1024**3)
            
            logger.info(f"ğŸ¯ æ£€æµ‹åˆ°GPU: {gpu_name} ({vram_gb:.1f}GB)")
            
            # æ ¹æ®GPUé€‰æ‹©é¦–é€‰æ¨¡å‹
            for gpu_key, models in self.config["preferred_models"].items():
                if gpu_key in gpu_name:
                    for model in models:
                        model_info = self.registry.get_model_info(model)
                        if model_info and model_info.min_vram_gb <= vram_gb * 0.8:  # ç•™20%ä½™é‡
                            logger.info(f"âœ… æ¨èæ¨¡å‹: {model}")
                            return model
            
            # é™çº§é€‰æ‹©
            recommended = self.registry.get_recommended_models(gpu_name, vram_gb * 0.8)
            if recommended:
                best_model = recommended[0].name
                logger.info(f"ğŸ’¡ è‡ªåŠ¨é€‰æ‹©æ¨¡å‹: {best_model}")
                return best_model
            
            return "tiny"  # å…œåº•é€‰æ‹©
            
        except Exception as e:
            logger.warning(f"æ¨¡å‹é€‰æ‹©å¤±è´¥: {e}")
            return self.config["default_model"]
    
    def list_available_models(self) -> List[Dict]:
        """åˆ—å‡ºå¯ç”¨æ¨¡å‹"""
        models = []
        for name, info in self.registry.models.items():
            models.append({
                "name": name,
                "type": info.model_type,
                "size": info.size,
                "language": info.language,
                "vram_gb": info.min_vram_gb,
                "tensorrt": info.tensorrt_support,
                "recommended_gpu": info.recommended_gpu
            })
        
        return sorted(models, key=lambda x: (x["type"], x["vram_gb"]))
    
    def validate_model(self, model_name: str) -> bool:
        """éªŒè¯æ¨¡å‹"""
        model_info = self.registry.get_model_info(model_name)
        if not model_info:
            return False
        
        # æ£€æŸ¥ç³»ç»Ÿå…¼å®¹æ€§
        try:
            import torch
            if torch.cuda.is_available():
                vram_gb = torch.cuda.get_device_properties(0).total_memory / (1024**3)
                if model_info.min_vram_gb > vram_gb:
                    logger.warning(f"âš ï¸ æ¨¡å‹ {model_name} éœ€è¦ {model_info.min_vram_gb}GB æ˜¾å­˜ï¼Œå½“å‰åªæœ‰ {vram_gb:.1f}GB")
                    return False
        except:
            pass
        
        return True
    
    def install_model(self, model_name: str) -> bool:
        """å®‰è£…æ¨¡å‹"""
        model_info = self.registry.get_model_info(model_name)
        if not model_info:
            logger.error(f"æœªçŸ¥æ¨¡å‹: {model_name}")
            return False
        
        if not self.validate_model(model_name):
            return False
        
        logger.info(f"ğŸ”„ å‡†å¤‡å®‰è£…æ¨¡å‹: {model_name}")
        # è¿™é‡Œå¯ä»¥æ·»åŠ å®é™…çš„æ¨¡å‹ä¸‹è½½é€»è¾‘
        return True

# å…¨å±€æ¨¡å‹ç®¡ç†å™¨å®ä¾‹
model_manager = ModelManager()

def get_model_manager() -> ModelManager:
    """è·å–æ¨¡å‹ç®¡ç†å™¨"""
    return model_manager
